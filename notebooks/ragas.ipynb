{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b9ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going at root folder\n",
    "import os\n",
    "os.chdir('../')\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7c9a1",
   "metadata": {},
   "source": [
    "## Langsmith setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f481d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith endpoint: https://api.smith.langchain.com\n",
      "Tracing enabled: true\n"
     ]
    }
   ],
   "source": [
    "## Langsmith setup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env into environment\n",
    "load_dotenv()\n",
    "\n",
    "# Access them\n",
    "api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "tracing = os.getenv(\"LANGSMITH_TRACING\")\n",
    "\n",
    "print(\"LangSmith endpoint:\", endpoint)\n",
    "print(\"Tracing enabled:\", tracing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d3783",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d8e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import json\n",
    "import dagshub\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "\n",
    "#extract text from pdf files   #load all the pdf files from data folder\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "293addeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"264Gaurav/medical-chatbot\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"264Gaurav/medical-chatbot\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository 264Gaurav/medical-chatbot initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository 264Gaurav/medical-chatbot initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run ID: e81f770d3d83424ab029f6431e56f23b\n",
      "MLflow Run ID: e81f770d3d83424ab029f6431e56f23b\n",
      "Loaded 637 documents in 8.87 seconds.\n",
      "MLflow run finished. View at https://dagshub.com/264Gaurav/medical-chatbot.mlflow\n",
      "ðŸƒ View run nervous-shrike-570 at: https://dagshub.com/264Gaurav/medical-chatbot.mlflow/#/experiments/1/runs/e81f770d3d83424ab029f6431e56f23b\n",
      "ðŸ§ª View experiment at: https://dagshub.com/264Gaurav/medical-chatbot.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "#mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "dagshub.init(repo_owner='264Gaurav', repo_name='medical-chatbot', mlflow=True)\n",
    "\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(\"RAG_ragas\")\n",
    "\n",
    "# Start an MLflow run to log this execution\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "\n",
    "    data_dir = 'data' # This is the path to your data directory\n",
    "    mlflow.log_param(\"data_directory\", data_dir)\n",
    "    mlflow.log_param(\"pdf_loader_class\", \"PyPDFLoader\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    extracted_data = load_pdf_files(data_dir) # extracted_data will be a list of Document objects\n",
    "    end_time = time.time()\n",
    "    loading_duration = end_time - start_time\n",
    "\n",
    "    mlflow.log_metric(\"num_documents_loaded\", len(extracted_data))\n",
    "    mlflow.log_metric(\"pdf_loading_duration_seconds\", loading_duration)\n",
    "\n",
    "    # Log the number of documents loaded - CORRECTED\n",
    "    # 'extracted_data' is the list of loaded documents, so its length gives the number of documents\n",
    "    num_documents = len(extracted_data)\n",
    "    mlflow.log_param(\"num_documents\", num_documents)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "    print(f\"Loaded {num_documents} documents in {loading_duration:.2f} seconds.\")\n",
    "    print(f\"MLflow run finished. View at {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec89be",
   "metadata": {},
   "source": [
    "### Filtering of Loaded DATA (i.e., extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3019aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "##DATA cleaning and filtering\n",
    "def filter_docs(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, filter out those with new list of Document objects\n",
    "    containing only 'source' and 'page' in metadata and the original page_content.\n",
    "\n",
    "    MLflow Tracking:\n",
    "    - Logs 'input_documents_count' as a metric.\n",
    "    - Logs 'output_documents_count' as a metric.\n",
    "    - Logs 'documents_filtered_count' as a metric.\n",
    "    - Logs 'filter_function_name' as a parameter.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Log initial state\n",
    "    input_documents_count = len(docs)\n",
    "    mlflow.log_metric(\"input_documents_count_for_filtering\", input_documents_count)\n",
    "    mlflow.log_param(\"filter_function_name\", \"filter_docs\")\n",
    "\n",
    "    minimal_docs: List[Document] = []\n",
    "\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get('source')\n",
    "        page = doc.metadata.get('page')\n",
    "        # Here, you might add more complex filtering logic if needed\n",
    "        # For this specific function, it always adds the document, just with minimal metadata\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src , \"page\":page}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    output_documents_count = len(minimal_docs)\n",
    "    documents_filtered_count = input_documents_count - output_documents_count # Will be 0 if all are kept\n",
    "\n",
    "    # Log metrics after processing\n",
    "    mlflow.log_metric(\"output_documents_count_after_filtering\", output_documents_count)\n",
    "    mlflow.log_metric(\"documents_filtered_count\", documents_filtered_count)\n",
    "\n",
    "    # Optional: Log a sample of filtered documents as an artifact\n",
    "    if minimal_docs:\n",
    "        sample_docs_path = \"filtered_docs_sample.json\"\n",
    "        # Log metadata from first few documents as a sample\n",
    "        sample_data = []\n",
    "        for i, doc in enumerate(minimal_docs[:5]): # Log first 5 documents\n",
    "            sample_data.append({\n",
    "                \"page_content_preview\": doc.page_content[:200] + \"...\",\n",
    "                \"metadata\": doc.metadata\n",
    "            })\n",
    "        with open(sample_docs_path, \"w\") as f:\n",
    "            json.dump(sample_data, f, indent=4)\n",
    "        mlflow.log_artifact(sample_docs_path, artifact_path=\"data_filtering_artifacts\")\n",
    "        os.remove(sample_docs_path) # Clean up local file\n",
    "\n",
    "    return minimal_docs\n",
    "\n",
    "\n",
    "minimal_docs = filter_docs(extracted_data) ## function calling here for filtering of extracted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d336a",
   "metadata": {},
   "source": [
    "### Split the Filtered Docs (minimal_docs) into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728e77df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/7jcjyd5s1cd09qhrzns_rny80000gn/T/ipykernel_90529/4157002724.py:27: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  json.dump([chunk.dict() for chunk in chunks[:5]], f, indent=4) # Log first 5 chunks\n"
     ]
    }
   ],
   "source": [
    "## split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(minimal_docs)\n",
    "\n",
    "\n",
    "chunks = text_split(minimal_docs)\n",
    "print(f\"Number of text chunks: {len(chunks)}\")\n",
    "\n",
    "\n",
    "# Log the number of chunks created\n",
    "num_chunks = len(chunks)\n",
    "mlflow.log_metric(\"num_chunks_created\", num_chunks) # Changed to metric as it's an output count\n",
    "\n",
    "# Log text splitter parameters\n",
    "mlflow.log_param(\"chunk_size\", chunk_size)\n",
    "mlflow.log_param(\"chunk_overlap\", chunk_overlap)\n",
    "mlflow.log_param(\"text_splitter_class\", \"RecursiveCharacterTextSplitter\")\n",
    "\n",
    "# Optional: Log a sample of the first few chunks as an artifact\n",
    "if chunks:\n",
    "    chunks_sample_path = \"chunks_sample.json\"\n",
    "    with open(chunks_sample_path, \"w\") as f:\n",
    "        json.dump([chunk.dict() for chunk in chunks[:5]], f, indent=4) # Log first 5 chunks\n",
    "    mlflow.log_artifact(chunks_sample_path, artifact_path=\"data_processing_artifacts\")\n",
    "    os.remove(chunks_sample_path) # Clean up local file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe0435",
   "metadata": {},
   "source": [
    "### Embedding model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788ef0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Ollama embedding model 'nomic-embed-text:latest' at http://localhost:11434 (init 0.01s).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import mlflow\n",
    "\n",
    "# Use the LangChain Ollama embedding class\n",
    "# If your LangChain installation exposes it under a different module, see the note below.\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "def download_embeddings(\n",
    "    model_name: str = \"nomic-embed-text:latest\",\n",
    "    base_url: str = \"http://localhost:11434\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Connects to the locally-running Ollama instance and returns an OllamaEmbeddings object\n",
    "    using the specified embedding model (nomic-embed-text:latest by default).\n",
    "    \"\"\"\n",
    "\n",
    "    # Log embedding model parameters\n",
    "    mlflow.log_param(\"embedding_model_name\", model_name)\n",
    "    mlflow.log_param(\"embedding_provider\", \"Ollama\")\n",
    "    mlflow.log_param(\"embedding_class\", \"OllamaEmbeddings\")\n",
    "    mlflow.log_param(\"ollama_base_url\", base_url)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Create the LangChain wrapper for Ollama embeddings\n",
    "    embeddings = OllamaEmbeddings(model=model_name, base_url=base_url)\n",
    "    end_time = time.time()\n",
    "    loading_duration = end_time - start_time\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"embedding_loading_duration_seconds\", loading_duration)\n",
    "\n",
    "    print(f\"Connected to Ollama embedding model '{model_name}' at {base_url} (init {loading_duration:.2f}s).\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "embedding = download_embeddings()\n",
    "## example: embedding.embed_documents([\"hello world\", \"another doc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c756ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length: 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embedding.embed_query(\"Hello to medical chatbot.\")\n",
    "\n",
    "vector_dim = len(vector);\n",
    "print(f\"Vector length: {vector_dim}\")  # Check the length of the vector  ## Dimentions of the vector\n",
    "\n",
    "mlflow.log_param(\"vector_embedding_size\", vector_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2382d6",
   "metadata": {},
   "source": [
    "### Vector DB setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bc61ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x173c344d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pinecone is the leading vector database for building accurate and performant AI applications\n",
    "from pinecone import Pinecone\n",
    "\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "pinecone_api = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api)\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbeada41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone index 'medical-chatbot' already exists. Connecting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index 'medical-chatbot' in 1.4292 seconds.\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "\n",
    "# Log stage name\n",
    "mlflow.log_param(\"stage\", \"pinecone_index_setup\")\n",
    "\n",
    "index_name = 'medical-chatbot'\n",
    "dimension = vector_dim  # Dimension of the embeddings (should match your embedding model output)\n",
    "metric = 'cosine' # Similarity metric\n",
    "\n",
    "# Log Pinecone index parameters\n",
    "mlflow.log_param(\"pinecone_index_name\", index_name)\n",
    "mlflow.log_param(\"pinecone_index_dimension\", dimension)\n",
    "mlflow.log_param(\"pinecone_index_metric\", metric)\n",
    "\n",
    "# Define the serverless spec parameters\n",
    "cloud_provider = \"aws\"\n",
    "region = \"us-east-1\"\n",
    "mlflow.log_param(\"pinecone_cloud_provider\", cloud_provider)\n",
    "mlflow.log_param(\"pinecone_region\", region)\n",
    "\n",
    "\n",
    "# Check if index exists and create if not\n",
    "start_check_time = time.time()\n",
    "index_exists = pc.has_index(index_name)\n",
    "end_check_time = time.time()\n",
    "check_duration = end_check_time - start_check_time\n",
    "\n",
    "mlflow.log_metric(\"pinecone_index_exists_check_duration_seconds\", check_duration)\n",
    "mlflow.log_param(\"pinecone_index_existed_before_run\", index_exists)\n",
    "\n",
    "if not index_exists:\n",
    "    mlflow.log_param(\"pinecone_index_action\", \"created_new_index\")\n",
    "    print(f\"Pinecone index '{index_name}' does not exist. Creating...\")\n",
    "    start_create_time = time.time()\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=metric,\n",
    "        spec=ServerlessSpec(cloud=cloud_provider, region=region)\n",
    "    )\n",
    "    end_create_time = time.time()\n",
    "    creation_duration = end_create_time - start_create_time\n",
    "    mlflow.log_metric(\"pinecone_index_creation_duration_seconds\", creation_duration)\n",
    "    print(f\"Pinecone index '{index_name}' created in {creation_duration:.2f} seconds.\")\n",
    "else:\n",
    "    mlflow.log_param(\"pinecone_index_action\", \"connected_to_existing_index\")\n",
    "    print(f\"Pinecone index '{index_name}' already exists. Connecting...\")\n",
    "    mlflow.log_metric(\"pinecone_index_creation_duration_seconds\", 0) # Log 0 if not created\n",
    "\n",
    "# Connect to the index\n",
    "start_connect_time = time.time()\n",
    "index = pc.Index(index_name)\n",
    "end_connect_time = time.time()\n",
    "connect_duration = end_connect_time - start_connect_time\n",
    "\n",
    "mlflow.log_metric(\"pinecone_index_connection_duration_seconds\", connect_duration)\n",
    "print(f\"Connected to Pinecone index '{index_name}' in {connect_duration:.4f} seconds.\")\n",
    "\n",
    "# Optional: Log some basic info about the connected index (e.g., number of vectors)\n",
    "# This might require querying the index, which adds time, so consider if needed for every run.\n",
    "try:\n",
    "    index_info = index.describe_index_stats()\n",
    "    mlflow.log_metric(\"pinecone_total_vector_count\", index_info.dimension) # This gives dimension, not vector count directly\n",
    "    # To get actual vector count:\n",
    "    if index_info.namespaces:\n",
    "        total_vectors_in_index = sum(ns.vector_count for ns_name, ns in index_info.namespaces.items())\n",
    "        mlflow.log_metric(\"pinecone_total_vectors_in_index\", total_vectors_in_index)\n",
    "        mlflow.log_param(\"pinecone_namespaces\", list(index_info.namespaces.keys()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not get Pinecone index stats: {e}\")\n",
    "    mlflow.log_param(\"pinecone_index_stats_error\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970fc6e",
   "metadata": {},
   "source": [
    "### Injection of filtered and chunked data (i.e., chunks) into Pinecone vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c09652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "def upsert_to_pinecone(chunks, embedding, index_name, batch_size: int = 50):\n",
    "    \"\"\"\n",
    "    Upserts documents to a Pinecone index in safe batches (<4MB per request).\n",
    "    Logs metadata and timing with MLflow.\n",
    "    \"\"\"\n",
    "    # Log input parameters\n",
    "    mlflow.log_param(\"vector_store_type\", \"PineconeVectorStore\")\n",
    "    mlflow.log_param(\"index_name_for_upsertion\", index_name)\n",
    "    mlflow.log_param(\"num_text_chunks_for_upsertion\", len(chunks))\n",
    "    mlflow.log_param(\"batch_size for pinecone data injection\", batch_size)\n",
    "    if hasattr(embedding, \"model\"):\n",
    "        mlflow.log_param(\"embedding_model_used_for_upsertion\", getattr(embedding, \"model\", \"unknown\"))\n",
    "\n",
    "    print(f\"Starting batched upsertion to Pinecone index '{index_name}' with {len(chunks)} chunks...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Upsert in batches to avoid Pinecone 4MB limit\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i : i + batch_size]\n",
    "        PineconeVectorStore.from_documents(\n",
    "            documents=batch,\n",
    "            embedding=embedding,\n",
    "            index_name=index_name\n",
    "        )\n",
    "        print(f\"  Upserted batch {i // batch_size + 1} ({len(batch)} chunks).\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    upsertion_duration = end_time - start_time\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"pinecone_upsertion_duration_seconds\", upsertion_duration)\n",
    "\n",
    "    print(f\"âœ… Upsertion to Pinecone completed in {upsertion_duration:.2f} seconds.\")\n",
    "\n",
    "\n",
    "\n",
    "# upsert_to_pinecone(chunks=chunks,  embedding=embedding, index_name=index_name, batch_size= 500)  ## uncomment only when you want to insert the data in pinecode DB (i.e., if data in DB not injected before)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8657ca",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c495022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents for 'What is Cancer?' in 28.8660 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load Existing index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "# Define parameters for retrieval\n",
    "retrieval_search_type = \"similarity\"\n",
    "retrieval_k = 5\n",
    "test_query = \"What is Cancer?\"\n",
    "\n",
    "# 1. Log Retrieval Parameters\n",
    "mlflow.log_param(\"retriever_search_type\", retrieval_search_type)\n",
    "mlflow.log_param(\"retriever_k_value\", retrieval_k)\n",
    "mlflow.log_param(\"retrieval_query\", test_query) # Log the specific query used\n",
    "\n",
    "# Configure the retriever\n",
    "retriever = docsearch.as_retriever(search_type=retrieval_search_type, search_kwargs={\"k\": retrieval_k})\n",
    "\n",
    "# 2. Measure and Log Retrieval Time on Test Query\n",
    "start_time = time.time()\n",
    "retrieved_docs = retriever.invoke(test_query) ## Retriever invoked\n",
    "end_time = time.time()\n",
    "retrieval_duration = end_time - start_time\n",
    "mlflow.log_metric(\"retrieval_duration_seconds\", retrieval_duration)\n",
    "\n",
    "# 3. Log Retrieved Document Count\n",
    "num_retrieved = len(retrieved_docs)\n",
    "mlflow.log_metric(\"num_retrieved_documents\", num_retrieved)\n",
    "\n",
    "\n",
    "# 4. Log a Sample of Retrieved Documents as an Artifact\n",
    "if retrieved_docs:\n",
    "    sample_docs = []\n",
    "    # Log details of the first 3 documents as a sample\n",
    "    for i, doc in enumerate(retrieved_docs[:3]):\n",
    "        sample_docs.append({\n",
    "            \"index\": i + 1,\n",
    "            \"source\": doc.metadata.get('source', 'N/A'),\n",
    "            \"page\": doc.metadata.get('page', 'N/A'),\n",
    "            \"content_preview\": doc.page_content[:200] + \"...\" # Log first 200 chars\n",
    "        })\n",
    "\n",
    "    temp_file_path = \"retrieved_docs_sample.json\"\n",
    "    with open(temp_file_path, \"w\") as f:\n",
    "        json.dump(sample_docs, f, indent=4)\n",
    "\n",
    "    mlflow.log_artifact(temp_file_path, artifact_path=\"retrieval_output_samples\")\n",
    "    os.remove(temp_file_path) # Clean up the local temporary file\n",
    "\n",
    "print(f\"Retrieved {num_retrieved} documents for '{test_query}' in {retrieval_duration:.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9601bbc",
   "metadata": {},
   "source": [
    "## LLM Model setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2d6f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama model: llama3.1:latest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/7jcjyd5s1cd09qhrzns_rny80000gn/T/ipykernel_90529/632503613.py:21: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  chatModel = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama model 'llama3.1:latest' initialized in 0.0011 seconds.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama  # wrapper for local Ollama models\n",
    "\n",
    "\n",
    "# Define LLM parameters\n",
    "llm_model_name = \"llama3.1:latest\"\n",
    "llm_temperature = 0\n",
    "llm_timeout = None\n",
    "llm_max_retries = 2\n",
    "\n",
    "# Log LLM parameters\n",
    "mlflow.log_param(\"llm_provider\", \"Ollama\")\n",
    "mlflow.log_param(\"llm_class\", \"Ollama\")\n",
    "mlflow.log_param(\"llm_model_name\", llm_model_name)\n",
    "mlflow.log_param(\"llm_temperature\", llm_temperature)\n",
    "mlflow.log_param(\"llm_timeout\", llm_timeout if llm_timeout is not None else \"None\")\n",
    "mlflow.log_param(\"llm_max_retries\", llm_max_retries)\n",
    "\n",
    "\n",
    "print(f\"Initializing Ollama model: {llm_model_name}...\")\n",
    "start_time = time.time()\n",
    "chatModel = Ollama(\n",
    "    model=llm_model_name,\n",
    "    temperature=llm_temperature,\n",
    "    # Ollama wrapper doesnâ€™t use `max_tokens` or `timeout` exactly the same way as Google GenAI,\n",
    "    # but you can still pass client kwargs if needed.\n",
    ")\n",
    "end_time = time.time()\n",
    "initialization_duration = end_time - start_time\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"llm_initialization_duration_seconds\", initialization_duration)\n",
    "\n",
    "print(f\"Ollama model '{llm_model_name}' initialized in {initialization_duration:.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e5716dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MS Dhoni, also known as Mahendra Singh Dhoni, is a former Indian international cricketer who played for the Indian national team from 2004 to 2019. He was one of the most successful and popular cricketers in the world during his playing career.\\n\\nHere are some key facts about MS Dhoni:\\n\\n**Early Life**\\n\\nDhoni was born on July 7, 1981, in Ranchi, Jharkhand (then part of Bihar), India. His father, Pan Singh Dhoni, was a railway employee, and his mother, Devki Devi, was a homemaker.\\n\\n**Cricket Career**\\n\\nDhoni started playing cricket at the age of eight and quickly rose through the ranks to become one of the most successful wicket-keepers in Indian cricket history. He made his international debut in 2004 against Bangladesh and went on to play for India in all formats of the game (Test, ODI, T20).\\n\\n**Achievements**\\n\\nDhoni's achievements are numerous:\\n\\n1. **Captaincy**: Dhoni was a successful captain of the Indian team, leading them to several victories, including the 2011 Cricket World Cup and the 2007 ICC World Twenty20.\\n2. **Wicket-keeping**: He is considered one of the greatest wicket-keepers in cricket history, known for his exceptional skills behind the stumps and his ability to take crucial catches.\\n3. **Batting**: Dhoni was a skilled batsman who could play both aggressive and defensive shots with ease. He scored over 10,000 runs in international cricket.\\n4. **Leadership**: Dhoni's leadership qualities were highly praised by his teammates and opponents alike. He was known for his calm and composed demeanor under pressure.\\n\\n**Personal Life**\\n\\nDhoni is married to Sakshi Singh Rawat, a former model and beauty pageant winner. They have two daughters, Ziva and Saanvi.\\n\\n**Post-Retirement**\\n\\nAfter announcing his retirement from international cricket in 2019, Dhoni has remained involved in the sport as the captain of Chennai Super Kings (CSK) in the Indian Premier League (IPL). He is also a successful entrepreneur and has invested in several businesses, including a restaurant chain and a sports management company.\\n\\n**Legacy**\\n\\nMS Dhoni's legacy extends beyond his on-field achievements. He is widely regarded as one of the greatest cricketers of all time, known for his humility, dedication, and leadership qualities. His impact on Indian cricket and his fans will be remembered for generations to come.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = chatModel.invoke(\"who is MS Dhoni\")\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0585b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n",
      "\n",
      "\n",
      " Prompt template defined and logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## for ready-made RAG prompts\n",
    "from langchain import hub\n",
    "# Pull a pre-made RAG prompt from LangChain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "\n",
    "## System Prompt defining\n",
    "system_prompt = (\n",
    "    \"You are an Medical assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Keep the answer concise and understandable.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create the ChatPromptTemplate\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "mlflow.log_param(\"system prompt for llm :\", system_prompt)\n",
    "print(prompt)\n",
    "print(\"\\n\\n Prompt template defined and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba084952",
   "metadata": {},
   "source": [
    "### Defining RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe1d1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the full RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chatModel\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48fb7557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The usual dosage of vitamin D for adults is 400 mg, taken two to three times a day with meals. For children under six years old, the usual dosage is 50 mg per day, divided into several small doses. The dose may be different for different people, so it's best to check with a physician or pharmacist for the correct dosage.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question using the RAG chain\n",
    "response = rag_chain.invoke(\"What dose of vitamin-D should we take?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21da96",
   "metadata": {},
   "source": [
    "## RAGAS Setup:\n",
    "\n",
    "### GroundTruth dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "440beb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"question\",\n",
    "    description=\"a question about the context.\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b089e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "format_instructions = question_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe9641",
   "metadata": {},
   "source": [
    "### Question generator llm model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "759d8d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama model: llama3.1:latest...\n",
      "Ollama model 'llama3.1:latest' initialized in 0.0009 seconds.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama  # wrapper for local Ollama models\n",
    "\n",
    "\n",
    "# Define LLM parameters\n",
    "llm_model_name = \"llama3.1:latest\"\n",
    "llm_temperature = 0.5\n",
    "llm_timeout = None\n",
    "llm_max_retries = 2\n",
    "\n",
    "# Log LLM parameters\n",
    "mlflow.log_param(\"question generator llm_provider\", \"Ollama\")\n",
    "mlflow.log_param(\"question generator llm_model_name\", llm_model_name)\n",
    "mlflow.log_param(\"question generator llm_temperature\", llm_temperature)\n",
    "mlflow.log_param(\"question generator llm_timeout\", llm_timeout if llm_timeout is not None else \"None\")\n",
    "mlflow.log_param(\"question generator llm_max_retries\", llm_max_retries)\n",
    "\n",
    "\n",
    "print(f\"Initializing Ollama model: {llm_model_name}...\")\n",
    "start_time = time.time()\n",
    "question_generation_llm = Ollama(\n",
    "    model=llm_model_name,\n",
    "    temperature=llm_temperature,\n",
    "    # Ollama wrapper doesnâ€™t use `max_tokens` or `timeout` exactly the same way as Google GenAI,\n",
    "    # but you can still pass client kwargs if needed.\n",
    ")\n",
    "end_time = time.time()\n",
    "initialization_duration = end_time - start_time\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"llm_initialization_duration_seconds\", initialization_duration)\n",
    "\n",
    "print(f\"Ollama model '{llm_model_name}' initialized in {initialization_duration:.4f} seconds.\")\n",
    "\n",
    "\n",
    "bare_prompt_template = \"{content}\"\n",
    "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "250e283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a Professor of Medical University creating a test for advanced students.\n",
    "For each context, create a question that is specific to the context. Avoid generic questions.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=chunks[0],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "question_generation_chain = bare_template | question_generation_llm\n",
    "\n",
    "response = question_generation_chain.invoke({\"content\" : messages})\n",
    "response\n",
    "\n",
    "\n",
    "output_dict = question_output_parser.parse(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a288d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "What is the purpose of the GALE ENCYCLOPEDIA OF MEDICINE SECOND EDITION, based on the content provided?\n"
     ]
    }
   ],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a946de",
   "metadata": {},
   "source": [
    "### create question set from initial few chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4ab32ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:40<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm ## tqdm to show the progress bar of iterative or loops operation\n",
    "\n",
    "qac_triples = []\n",
    "\n",
    "for text in tqdm(chunks[:30]):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=text,\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = question_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = question_output_parser.parse(response)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  output_dict[\"context\"] = text\n",
    "  qac_triples.append(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0881aea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the primary approach of alternative medicine, as described in The Gale Encyclopedia of Medicine 2?',\n",
       " 'context': Document(metadata={'source': 'data/Medical_book.pdf', 'page': 6}, page_content='Diagnosis Description\\nTreatment Preparation\\nAlternative treatment Aftercare\\nPrognosis Risks\\nPrevention Normal/Abnormal results\\nResources Resources\\nKey terms Key terms\\nIn recent years there has been a resurgence of interest\\nin holistic medicine that emphasizes the connection\\nbetween mind and body. Aimed at achieving and main-\\ntaining good health rather than just eliminating disease,\\nthis approach has come to be known as alternative medi-\\ncine. The Gale Encyclopedia of Medicine 2 includes a')}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qac_triples[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e28f17",
   "metadata": {},
   "source": [
    "### Answer generator llm model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0e78662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama model: llama3.1:latest...\n",
      "Ollama model 'llama3.1:latest' initialized in 0.0006 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Define LLM parameters\n",
    "llm_model_name = \"llama3.1:latest\"\n",
    "llm_temperature = 0.5\n",
    "llm_timeout = None\n",
    "llm_max_retries = 2\n",
    "\n",
    "# Log LLM parameters\n",
    "mlflow.log_param(\"answer generator llm_provider\", \"Ollama\")\n",
    "mlflow.log_param(\"answer generator llm_model_name\", llm_model_name)\n",
    "mlflow.log_param(\"answer generator llm_temperature\", llm_temperature)\n",
    "mlflow.log_param(\"answer generator llm_timeout\", llm_timeout if llm_timeout is not None else \"None\")\n",
    "mlflow.log_param(\"answer generator llm_max_retries\", llm_max_retries)\n",
    "\n",
    "\n",
    "print(f\"Initializing Ollama model: {llm_model_name}...\")\n",
    "start_time = time.time()\n",
    "answer_generation_llm = Ollama(\n",
    "    model=llm_model_name,\n",
    "    temperature=llm_temperature,\n",
    ")\n",
    "end_time = time.time()\n",
    "initialization_duration = end_time - start_time\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"llm_initialization_duration_seconds\", initialization_duration)\n",
    "\n",
    "print(f\"Ollama model '{llm_model_name}' initialized in {initialization_duration:.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f2d5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "format_instructions = answer_output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a Professor of Medical university creating a test for advanced students. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=qac_triples[20][\"context\"],\n",
    "    question=qac_triples[20][\"question\"],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "\n",
    "answer_generation_chain = bare_template | answer_generation_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d361f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "output_dict = answer_output_parser.parse(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33ba1c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\n",
      "Holistic medicine that emphasizes the connection between mind and body, aiming to achieve and maintain good health rather than just eliminating disease.\n"
     ]
    }
   ],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f6e4b",
   "metadata": {},
   "source": [
    "### Generating answer for all the question generated earlier (in earlier section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f534544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [02:34<00:00,  5.15s/it]\n"
     ]
    }
   ],
   "source": [
    "for triple in tqdm(qac_triples):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=triple[\"context\"],\n",
    "      question=triple[\"question\"],\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = answer_output_parser.parse(response)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  triple[\"answer\"] = output_dict[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55868916",
   "metadata": {},
   "source": [
    "### Now, save the generated question , ground truth answer in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "313a6c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the name of this medical reference boo...</td>\n",
       "      <td>The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND ED...</td>\n",
       "      <td>The GALE ENCYCLOPEDIA of MEDICINE SECOND EDITION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the name of the editor responsible for...</td>\n",
       "      <td>The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND ED...</td>\n",
       "      <td>Jacqueline L. Longe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is the Project Manager for the Imaging and...</td>\n",
       "      <td>STAFF\\nJacqueline L. Longe, Project Editor\\nDe...</td>\n",
       "      <td>Robyn V. Young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the role of Kelly A. Quin in relation ...</td>\n",
       "      <td>Multimedia Content\\nDean Dauphinais, Senior Ed...</td>\n",
       "      <td>Kelly A. Quin is the Editor of Imaging and Mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the title of the medical encyclopedia ...</td>\n",
       "      <td>Maria Franklin, Permissions Manager\\nMargaret ...</td>\n",
       "      <td>The Gale Encyclopedia of Medicine, Second Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the name of the publisher responsible ...</td>\n",
       "      <td>of MEDICINE\\nSECOND EDITION\\nSince this page c...</td>\n",
       "      <td>The publisher responsible for releasing this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What type of paper is used for printing this m...</td>\n",
       "      <td>ty for errors, omissions or discrepancies. The...</td>\n",
       "      <td>Recycled paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What standard does the paper used in this publ...</td>\n",
       "      <td>This book is printed on recycled paper that me...</td>\n",
       "      <td>American National Standard for Information Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the purpose of adding value to the und...</td>\n",
       "      <td>petition, and other applicable laws. The autho...</td>\n",
       "      <td>To provide a unique and original perspective o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the ISBN number for Volume 1 of this m...</td>\n",
       "      <td>Copyright Â© 2002\\nGale Group\\n27500 Drake Road...</td>\n",
       "      <td>{'question': 'What is the ISBN number for Volu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the purpose of including bibliographic...</td>\n",
       "      <td>Gale encyclopedia of medicine / Jacqueline L. ...</td>\n",
       "      <td>To provide credibility and authenticity to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the title of this medical encyclopedia...</td>\n",
       "      <td>(vol. 5)\\n1. Internal medicineâ€”Encyclopedias. ...</td>\n",
       "      <td>Internal medicineâ€”Encyclopedias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the purpose of listing the contributor...</td>\n",
       "      <td>Introduction.....................................</td>\n",
       "      <td>The purpose of listing contributors in a medic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of Volume 5 in this medica...</td>\n",
       "      <td>Volume 4: N-S....................................</td>\n",
       "      <td>To provide a comprehensive index of medical te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the primary purpose of The Gale Encycl...</td>\n",
       "      <td>The Gale Encyclopedia of Medicine 2is a medica...</td>\n",
       "      <td>To inform and educate readers about a wide var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What type of warranty does the Gale Group make...</td>\n",
       "      <td>Group has made substantial efforts to provide ...</td>\n",
       "      <td>The Gale Group makes no representations or war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the primary purpose of including a dis...</td>\n",
       "      <td>should be aware that the universe of medical k...</td>\n",
       "      <td>To inform readers that the medical knowledge p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the primary purpose of The Gale Encycl...</td>\n",
       "      <td>The Gale Encyclopedia of Medicine 2 (GEM2) is ...</td>\n",
       "      <td>The primary purpose of The Gale Encyclopedia o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the primary purpose of the Gale Encycl...</td>\n",
       "      <td>Medicine 2 fills a gap between basic consumer ...</td>\n",
       "      <td>To provide a comprehensive resource that bridg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the primary purpose of acetaminophen (...</td>\n",
       "      <td>parentheses, eg. acetaminophen (Tylenol). Thro...</td>\n",
       "      <td>To relieve pain and reduce fever.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What is the primary approach of alternative me...</td>\n",
       "      <td>Diagnosis Description\\nTreatment Preparation\\n...</td>\n",
       "      <td>Holistic medicine that emphasizes the connecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the primary difference between traditi...</td>\n",
       "      <td>cine. The Gale Encyclopedia of Medicine 2 incl...</td>\n",
       "      <td>Traditional Chinese medicine focuses on restor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is the primary source of the preliminary ...</td>\n",
       "      <td>INCLUSION CRITERIA\\nA preliminary list of dise...</td>\n",
       "      <td>professional medical guides and textbooks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What type of professionals contributed to the ...</td>\n",
       "      <td>by category and sent to GEM2 medical advisors,...</td>\n",
       "      <td>experienced medical writers, including physici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What is the primary organizational feature of ...</td>\n",
       "      <td>reviewed the completed essays to insure that t...</td>\n",
       "      <td>Straight alphabetical arrangement and bold-fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What is the primary purpose of cross-reference...</td>\n",
       "      <td>â€¢ Cross-references placed throughout the encyc...</td>\n",
       "      <td>To provide readers with information on subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What is the primary purpose of the Resources s...</td>\n",
       "      <td>appendix contains an extensive list of organiz...</td>\n",
       "      <td>The primary purpose of the Resources section i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What is the role of Dr. Richard Adrouny in the...</td>\n",
       "      <td>MEDICAL ADVISORS\\nA. Richard Adrouny, M.D.,\\nF...</td>\n",
       "      <td>Dr. Richard Adrouny is a Clinical Assistant Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the highest academic degree held by La...</td>\n",
       "      <td>Rosalyn Carson-DeWitt, M.D.\\nDurham, NC\\nRobin...</td>\n",
       "      <td>M.Sc., M.Sc.(MedSci), MSA, Msc.Psych., MRSNZ F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What is the role of Dr. Ira Michelson in this ...</td>\n",
       "      <td>Larry I. Lutwick M.D., F.A.C.P.\\nDirector, Inf...</td>\n",
       "      <td>Dr. Ira Michelson is a Physician and Clinical ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the name of this medical reference boo...   \n",
       "1   What is the name of the editor responsible for...   \n",
       "2   Who is the Project Manager for the Imaging and...   \n",
       "3   What is the role of Kelly A. Quin in relation ...   \n",
       "4   What is the title of the medical encyclopedia ...   \n",
       "5   What is the name of the publisher responsible ...   \n",
       "6   What type of paper is used for printing this m...   \n",
       "7   What standard does the paper used in this publ...   \n",
       "8   What is the purpose of adding value to the und...   \n",
       "9   What is the ISBN number for Volume 1 of this m...   \n",
       "10  What is the purpose of including bibliographic...   \n",
       "11  What is the title of this medical encyclopedia...   \n",
       "12  What is the purpose of listing the contributor...   \n",
       "13  What is the purpose of Volume 5 in this medica...   \n",
       "14  What is the primary purpose of The Gale Encycl...   \n",
       "15  What type of warranty does the Gale Group make...   \n",
       "16  What is the primary purpose of including a dis...   \n",
       "17  What is the primary purpose of The Gale Encycl...   \n",
       "18  What is the primary purpose of the Gale Encycl...   \n",
       "19  What is the primary purpose of acetaminophen (...   \n",
       "20  What is the primary approach of alternative me...   \n",
       "21  What is the primary difference between traditi...   \n",
       "22  What is the primary source of the preliminary ...   \n",
       "23  What type of professionals contributed to the ...   \n",
       "24  What is the primary organizational feature of ...   \n",
       "25  What is the primary purpose of cross-reference...   \n",
       "26  What is the primary purpose of the Resources s...   \n",
       "27  What is the role of Dr. Richard Adrouny in the...   \n",
       "28  What is the highest academic degree held by La...   \n",
       "29  What is the role of Dr. Ira Michelson in this ...   \n",
       "\n",
       "                                              context  \\\n",
       "0   The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND ED...   \n",
       "1   The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND ED...   \n",
       "2   STAFF\\nJacqueline L. Longe, Project Editor\\nDe...   \n",
       "3   Multimedia Content\\nDean Dauphinais, Senior Ed...   \n",
       "4   Maria Franklin, Permissions Manager\\nMargaret ...   \n",
       "5   of MEDICINE\\nSECOND EDITION\\nSince this page c...   \n",
       "6   ty for errors, omissions or discrepancies. The...   \n",
       "7   This book is printed on recycled paper that me...   \n",
       "8   petition, and other applicable laws. The autho...   \n",
       "9   Copyright Â© 2002\\nGale Group\\n27500 Drake Road...   \n",
       "10  Gale encyclopedia of medicine / Jacqueline L. ...   \n",
       "11  (vol. 5)\\n1. Internal medicineâ€”Encyclopedias. ...   \n",
       "12  Introduction.....................................   \n",
       "13  Volume 4: N-S....................................   \n",
       "14  The Gale Encyclopedia of Medicine 2is a medica...   \n",
       "15  Group has made substantial efforts to provide ...   \n",
       "16  should be aware that the universe of medical k...   \n",
       "17  The Gale Encyclopedia of Medicine 2 (GEM2) is ...   \n",
       "18  Medicine 2 fills a gap between basic consumer ...   \n",
       "19  parentheses, eg. acetaminophen (Tylenol). Thro...   \n",
       "20  Diagnosis Description\\nTreatment Preparation\\n...   \n",
       "21  cine. The Gale Encyclopedia of Medicine 2 incl...   \n",
       "22  INCLUSION CRITERIA\\nA preliminary list of dise...   \n",
       "23  by category and sent to GEM2 medical advisors,...   \n",
       "24  reviewed the completed essays to insure that t...   \n",
       "25  â€¢ Cross-references placed throughout the encyc...   \n",
       "26  appendix contains an extensive list of organiz...   \n",
       "27  MEDICAL ADVISORS\\nA. Richard Adrouny, M.D.,\\nF...   \n",
       "28  Rosalyn Carson-DeWitt, M.D.\\nDurham, NC\\nRobin...   \n",
       "29  Larry I. Lutwick M.D., F.A.C.P.\\nDirector, Inf...   \n",
       "\n",
       "                                         ground_truth  \n",
       "0    The GALE ENCYCLOPEDIA of MEDICINE SECOND EDITION  \n",
       "1                                 Jacqueline L. Longe  \n",
       "2                                      Robyn V. Young  \n",
       "3   Kelly A. Quin is the Editor of Imaging and Mul...  \n",
       "4   The Gale Encyclopedia of Medicine, Second Edition  \n",
       "5   The publisher responsible for releasing this m...  \n",
       "6                                      Recycled paper  \n",
       "7   American National Standard for Information Sci...  \n",
       "8   To provide a unique and original perspective o...  \n",
       "9   {'question': 'What is the ISBN number for Volu...  \n",
       "10  To provide credibility and authenticity to the...  \n",
       "11                    Internal medicineâ€”Encyclopedias  \n",
       "12  The purpose of listing contributors in a medic...  \n",
       "13  To provide a comprehensive index of medical te...  \n",
       "14  To inform and educate readers about a wide var...  \n",
       "15  The Gale Group makes no representations or war...  \n",
       "16  To inform readers that the medical knowledge p...  \n",
       "17  The primary purpose of The Gale Encyclopedia o...  \n",
       "18  To provide a comprehensive resource that bridg...  \n",
       "19                  To relieve pain and reduce fever.  \n",
       "20  Holistic medicine that emphasizes the connecti...  \n",
       "21  Traditional Chinese medicine focuses on restor...  \n",
       "22          professional medical guides and textbooks  \n",
       "23  experienced medical writers, including physici...  \n",
       "24  Straight alphabetical arrangement and bold-fac...  \n",
       "25  To provide readers with information on subject...  \n",
       "26  The primary purpose of the Resources section i...  \n",
       "27  Dr. Richard Adrouny is a Clinical Assistant Pr...  \n",
       "28  M.Sc., M.Sc.(MedSci), MSA, Msc.Psych., MRSNZ F...  \n",
       "29  Dr. Ira Michelson is a Physician and Clinical ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
    "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
    "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
    "\n",
    "ground_truth_qac_set\n",
    "# eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5224f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
