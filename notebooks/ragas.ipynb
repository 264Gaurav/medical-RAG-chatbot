{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b9ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going at root folder\n",
    "import os\n",
    "os.chdir('../')\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7c9a1",
   "metadata": {},
   "source": [
    "## Langsmith setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f481d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith endpoint: https://api.smith.langchain.com\n",
      "Tracing enabled: true\n"
     ]
    }
   ],
   "source": [
    "## Langsmith setup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env into environment\n",
    "load_dotenv()\n",
    "\n",
    "# Access them\n",
    "api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "tracing = os.getenv(\"LANGSMITH_TRACING\")\n",
    "\n",
    "print(\"LangSmith endpoint:\", endpoint)\n",
    "print(\"Tracing enabled:\", tracing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d3783",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65d8e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import json\n",
    "import dagshub\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "\n",
    "#extract text from pdf files   #load all the pdf files from data folder\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "293addeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"264Gaurav/medical-chatbot\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"264Gaurav/medical-chatbot\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository 264Gaurav/medical-chatbot initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository 264Gaurav/medical-chatbot initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MlflowException",
     "evalue": "API request to endpoint /api/2.0/mlflow/experiments/get-by-name failed with error code 403 != 200. Response body: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m dagshub.init(repo_owner=\u001b[33m'\u001b[39m\u001b[33m264Gaurav\u001b[39m\u001b[33m'\u001b[39m, repo_name=\u001b[33m'\u001b[39m\u001b[33mmedical-chatbot\u001b[39m\u001b[33m'\u001b[39m, mlflow=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Set the experiment name\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRAG_ragas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Start an MLflow run to log this execution\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run() \u001b[38;5;28;01mas\u001b[39;00m run:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medical/lib/python3.12/site-packages/mlflow/tracking/fluent.py:157\u001b[39m, in \u001b[36mset_experiment\u001b[39m\u001b[34m(experiment_name, experiment_id)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _experiment_lock:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         experiment = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment:\n\u001b[32m    159\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medical/lib/python3.12/site-packages/mlflow/tracking/client.py:1703\u001b[39m, in \u001b[36mMlflowClient.get_experiment_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Optional[Experiment]:\n\u001b[32m   1672\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[32m   1673\u001b[39m \n\u001b[32m   1674\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1701\u001b[39m \u001b[33;03m        Lifecycle_stage: active\u001b[39;00m\n\u001b[32m   1702\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medical/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/client.py:595\u001b[39m, in \u001b[36mTrackingServiceClient.get_experiment_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_experiment_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m    588\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    589\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    590\u001b[39m \u001b[33;03m        name: The experiment name.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    593\u001b[39m \u001b[33;03m        :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[32m    594\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medical/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:641\u001b[39m, in \u001b[36mRestStore.get_experiment_by_name\u001b[39m\u001b[34m(self, experiment_name)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    640\u001b[39m     req_body = message_to_json(GetExperimentByName(experiment_name=experiment_name))\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m     response_proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Experiment.from_proto(response_proto.experiment)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medical/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:90\u001b[39m, in \u001b[36mRestStore._call_endpoint\u001b[39m\u001b[34m(self, api, json_body, endpoint)\u001b[39m\n\u001b[32m     88\u001b[39m     endpoint, method = _METHOD_TO_INFO[api]\n\u001b[32m     89\u001b[39m response_proto = api.Response()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medical/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:402\u001b[39m, in \u001b[36mcall_endpoint\u001b[39m\u001b[34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[39m\n\u001b[32m    399\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m] = json_body\n\u001b[32m    400\u001b[39m     response = http_request(**call_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m response = \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m response_to_parse = response.text\n\u001b[32m    404\u001b[39m js_dict = json.loads(response_to_parse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/medical/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:265\u001b[39m, in \u001b[36mverify_rest_response\u001b[39m\u001b[34m(response, endpoint)\u001b[39m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    261\u001b[39m         base_msg = (\n\u001b[32m    262\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    263\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != 200\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    264\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    266\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Response body: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    267\u001b[39m             error_code=get_error_code(response.status_code),\n\u001b[32m    268\u001b[39m         )\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Skip validation for endpoints (e.g. DBFS file-download API) which may return a non-JSON\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# response\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint.startswith(_REST_API_PATH_PREFIX) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _can_parse_as_json_object(response.text):\n",
      "\u001b[31mMlflowException\u001b[39m: API request to endpoint /api/2.0/mlflow/experiments/get-by-name failed with error code 403 != 200. Response body: ''"
     ]
    }
   ],
   "source": [
    "#mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "dagshub.init(repo_owner='264Gaurav', repo_name='medical-chatbot', mlflow=True)\n",
    "\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(\"RAG_ragas\")\n",
    "\n",
    "# Start an MLflow run to log this execution\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "\n",
    "    data_dir = 'data' # This is the path to your data directory\n",
    "    mlflow.log_param(\"data_directory\", data_dir)\n",
    "    mlflow.log_param(\"pdf_loader_class\", \"PyPDFLoader\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    extracted_data = load_pdf_files(data_dir) # extracted_data will be a list of Document objects\n",
    "    end_time = time.time()\n",
    "    loading_duration = end_time - start_time\n",
    "\n",
    "    mlflow.log_metric(\"num_documents_loaded\", len(extracted_data))\n",
    "    mlflow.log_metric(\"pdf_loading_duration_seconds\", loading_duration)\n",
    "\n",
    "    # Log the number of documents loaded - CORRECTED\n",
    "    # 'extracted_data' is the list of loaded documents, so its length gives the number of documents\n",
    "    num_documents = len(extracted_data)\n",
    "    mlflow.log_param(\"num_documents\", num_documents)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "    print(f\"Loaded {num_documents} documents in {loading_duration:.2f} seconds.\")\n",
    "    print(f\"MLflow run finished. View at {mlflow.get_tracking_uri()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec89be",
   "metadata": {},
   "source": [
    "### Filtering of Loaded DATA (i.e., extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3019aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "##DATA cleaning and filtering\n",
    "def filter_docs(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, filter out those with new list of Document objects\n",
    "    containing only 'source' and 'page' in metadata and the original page_content.\n",
    "\n",
    "    MLflow Tracking:\n",
    "    - Logs 'input_documents_count' as a metric.\n",
    "    - Logs 'output_documents_count' as a metric.\n",
    "    - Logs 'documents_filtered_count' as a metric.\n",
    "    - Logs 'filter_function_name' as a parameter.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Log initial state\n",
    "    input_documents_count = len(docs)\n",
    "    mlflow.log_metric(\"input_documents_count_for_filtering\", input_documents_count)\n",
    "    mlflow.log_param(\"filter_function_name\", \"filter_docs\")\n",
    "\n",
    "    minimal_docs: List[Document] = []\n",
    "\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get('source')\n",
    "        page = doc.metadata.get('page')\n",
    "        # Here, you might add more complex filtering logic if needed\n",
    "        # For this specific function, it always adds the document, just with minimal metadata\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src , \"page\":page}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    output_documents_count = len(minimal_docs)\n",
    "    documents_filtered_count = input_documents_count - output_documents_count # Will be 0 if all are kept\n",
    "\n",
    "    # Log metrics after processing\n",
    "    mlflow.log_metric(\"output_documents_count_after_filtering\", output_documents_count)\n",
    "    mlflow.log_metric(\"documents_filtered_count\", documents_filtered_count)\n",
    "\n",
    "    # Optional: Log a sample of filtered documents as an artifact\n",
    "    if minimal_docs:\n",
    "        sample_docs_path = \"filtered_docs_sample.json\"\n",
    "        # Log metadata from first few documents as a sample\n",
    "        sample_data = []\n",
    "        for i, doc in enumerate(minimal_docs[:5]): # Log first 5 documents\n",
    "            sample_data.append({\n",
    "                \"page_content_preview\": doc.page_content[:200] + \"...\",\n",
    "                \"metadata\": doc.metadata\n",
    "            })\n",
    "        with open(sample_docs_path, \"w\") as f:\n",
    "            json.dump(sample_data, f, indent=4)\n",
    "        mlflow.log_artifact(sample_docs_path, artifact_path=\"data_filtering_artifacts\")\n",
    "        os.remove(sample_docs_path) # Clean up local file\n",
    "\n",
    "    return minimal_docs\n",
    "\n",
    "\n",
    "minimal_docs = filter_docs(extracted_data) ## function calling here for filtering of extracted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d336a",
   "metadata": {},
   "source": [
    "### Split the Filtered Docs (minimal_docs) into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728e77df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/7jcjyd5s1cd09qhrzns_rny80000gn/T/ipykernel_67318/4157002724.py:27: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  json.dump([chunk.dict() for chunk in chunks[:5]], f, indent=4) # Log first 5 chunks\n"
     ]
    }
   ],
   "source": [
    "## split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(minimal_docs)\n",
    "\n",
    "\n",
    "chunks = text_split(minimal_docs)\n",
    "print(f\"Number of text chunks: {len(chunks)}\")\n",
    "\n",
    "\n",
    "# Log the number of chunks created\n",
    "num_chunks = len(chunks)\n",
    "mlflow.log_metric(\"num_chunks_created\", num_chunks) # Changed to metric as it's an output count\n",
    "\n",
    "# Log text splitter parameters\n",
    "mlflow.log_param(\"chunk_size\", chunk_size)\n",
    "mlflow.log_param(\"chunk_overlap\", chunk_overlap)\n",
    "mlflow.log_param(\"text_splitter_class\", \"RecursiveCharacterTextSplitter\")\n",
    "\n",
    "# Optional: Log a sample of the first few chunks as an artifact\n",
    "if chunks:\n",
    "    chunks_sample_path = \"chunks_sample.json\"\n",
    "    with open(chunks_sample_path, \"w\") as f:\n",
    "        json.dump([chunk.dict() for chunk in chunks[:5]], f, indent=4) # Log first 5 chunks\n",
    "    mlflow.log_artifact(chunks_sample_path, artifact_path=\"data_processing_artifacts\")\n",
    "    os.remove(chunks_sample_path) # Clean up local file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe0435",
   "metadata": {},
   "source": [
    "### Embedding model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788ef0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Ollama embedding model 'nomic-embed-text:latest' at http://localhost:11434 (init 0.01s).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import mlflow\n",
    "\n",
    "# Use the LangChain Ollama embedding class\n",
    "# If your LangChain installation exposes it under a different module, see the note below.\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "def download_embeddings(\n",
    "    model_name: str = \"nomic-embed-text:latest\",\n",
    "    base_url: str = \"http://localhost:11434\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Connects to the locally-running Ollama instance and returns an OllamaEmbeddings object\n",
    "    using the specified embedding model (nomic-embed-text:latest by default).\n",
    "    \"\"\"\n",
    "\n",
    "    # Log embedding model parameters\n",
    "    mlflow.log_param(\"embedding_model_name\", model_name)\n",
    "    mlflow.log_param(\"embedding_provider\", \"Ollama\")\n",
    "    mlflow.log_param(\"embedding_class\", \"OllamaEmbeddings\")\n",
    "    mlflow.log_param(\"ollama_base_url\", base_url)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Create the LangChain wrapper for Ollama embeddings\n",
    "    embeddings = OllamaEmbeddings(model=model_name, base_url=base_url)\n",
    "    end_time = time.time()\n",
    "    loading_duration = end_time - start_time\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"embedding_loading_duration_seconds\", loading_duration)\n",
    "\n",
    "    print(f\"Connected to Ollama embedding model '{model_name}' at {base_url} (init {loading_duration:.2f}s).\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "embedding = download_embeddings()\n",
    "## example: embedding.embed_documents([\"hello world\", \"another doc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c756ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length: 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embedding.embed_query(\"Hello to medical chatbot.\")\n",
    "\n",
    "vector_dim = len(vector);\n",
    "print(f\"Vector length: {vector_dim}\")  # Check the length of the vector  ## Dimentions of the vector\n",
    "\n",
    "mlflow.log_param(\"vector_embedding_size\", vector_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2382d6",
   "metadata": {},
   "source": [
    "### Vector DB setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bc61ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x167f1e3f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pinecone is the leading vector database for building accurate and performant AI applications\n",
    "from pinecone import Pinecone\n",
    "\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "pinecone_api = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api)\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbeada41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone index 'medical-chatbot' does not exist. Creating...\n",
      "Pinecone index 'medical-chatbot' created in 11.97 seconds.\n",
      "Connected to Pinecone index 'medical-chatbot' in 0.0265 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "\n",
    "# Log stage name\n",
    "mlflow.log_param(\"stage\", \"pinecone_index_setup\")\n",
    "\n",
    "index_name = 'medical-chatbot'\n",
    "dimension = vector_dim  # Dimension of the embeddings (should match your embedding model output)\n",
    "metric = 'cosine' # Similarity metric\n",
    "\n",
    "# Log Pinecone index parameters\n",
    "mlflow.log_param(\"pinecone_index_name\", index_name)\n",
    "mlflow.log_param(\"pinecone_index_dimension\", dimension)\n",
    "mlflow.log_param(\"pinecone_index_metric\", metric)\n",
    "\n",
    "# Define the serverless spec parameters\n",
    "cloud_provider = \"aws\"\n",
    "region = \"us-east-1\"\n",
    "mlflow.log_param(\"pinecone_cloud_provider\", cloud_provider)\n",
    "mlflow.log_param(\"pinecone_region\", region)\n",
    "\n",
    "\n",
    "# Check if index exists and create if not\n",
    "start_check_time = time.time()\n",
    "index_exists = pc.has_index(index_name)\n",
    "end_check_time = time.time()\n",
    "check_duration = end_check_time - start_check_time\n",
    "\n",
    "mlflow.log_metric(\"pinecone_index_exists_check_duration_seconds\", check_duration)\n",
    "mlflow.log_param(\"pinecone_index_existed_before_run\", index_exists)\n",
    "\n",
    "if not index_exists:\n",
    "    mlflow.log_param(\"pinecone_index_action\", \"created_new_index\")\n",
    "    print(f\"Pinecone index '{index_name}' does not exist. Creating...\")\n",
    "    start_create_time = time.time()\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=metric,\n",
    "        spec=ServerlessSpec(cloud=cloud_provider, region=region)\n",
    "    )\n",
    "    end_create_time = time.time()\n",
    "    creation_duration = end_create_time - start_create_time\n",
    "    mlflow.log_metric(\"pinecone_index_creation_duration_seconds\", creation_duration)\n",
    "    print(f\"Pinecone index '{index_name}' created in {creation_duration:.2f} seconds.\")\n",
    "else:\n",
    "    mlflow.log_param(\"pinecone_index_action\", \"connected_to_existing_index\")\n",
    "    print(f\"Pinecone index '{index_name}' already exists. Connecting...\")\n",
    "    mlflow.log_metric(\"pinecone_index_creation_duration_seconds\", 0) # Log 0 if not created\n",
    "\n",
    "# Connect to the index\n",
    "start_connect_time = time.time()\n",
    "index = pc.Index(index_name)\n",
    "end_connect_time = time.time()\n",
    "connect_duration = end_connect_time - start_connect_time\n",
    "\n",
    "mlflow.log_metric(\"pinecone_index_connection_duration_seconds\", connect_duration)\n",
    "print(f\"Connected to Pinecone index '{index_name}' in {connect_duration:.4f} seconds.\")\n",
    "\n",
    "# Optional: Log some basic info about the connected index (e.g., number of vectors)\n",
    "# This might require querying the index, which adds time, so consider if needed for every run.\n",
    "try:\n",
    "    index_info = index.describe_index_stats()\n",
    "    mlflow.log_metric(\"pinecone_total_vector_count\", index_info.dimension) # This gives dimension, not vector count directly\n",
    "    # To get actual vector count:\n",
    "    if index_info.namespaces:\n",
    "        total_vectors_in_index = sum(ns.vector_count for ns_name, ns in index_info.namespaces.items())\n",
    "        mlflow.log_metric(\"pinecone_total_vectors_in_index\", total_vectors_in_index)\n",
    "        mlflow.log_param(\"pinecone_namespaces\", list(index_info.namespaces.keys()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not get Pinecone index stats: {e}\")\n",
    "    mlflow.log_param(\"pinecone_index_stats_error\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970fc6e",
   "metadata": {},
   "source": [
    "### Injection of filtered and chunked data (i.e., chunks) into Pinecone vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c09652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "def upsert_to_pinecone(chunks, embedding, index_name, batch_size: int = 50):\n",
    "    \"\"\"\n",
    "    Upserts documents to a Pinecone index in safe batches (<4MB per request).\n",
    "    Logs metadata and timing with MLflow.\n",
    "    \"\"\"\n",
    "    # Log input parameters\n",
    "    mlflow.log_param(\"vector_store_type\", \"PineconeVectorStore\")\n",
    "    mlflow.log_param(\"index_name_for_upsertion\", index_name)\n",
    "    mlflow.log_param(\"num_text_chunks_for_upsertion\", len(chunks))\n",
    "    mlflow.log_param(\"batch_size for pinecone data injection\", batch_size)\n",
    "    if hasattr(embedding, \"model\"):\n",
    "        mlflow.log_param(\"embedding_model_used_for_upsertion\", getattr(embedding, \"model\", \"unknown\"))\n",
    "\n",
    "    print(f\"Starting batched upsertion to Pinecone index '{index_name}' with {len(chunks)} chunks...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Upsert in batches to avoid Pinecone 4MB limit\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i : i + batch_size]\n",
    "        PineconeVectorStore.from_documents(\n",
    "            documents=batch,\n",
    "            embedding=embedding,\n",
    "            index_name=index_name\n",
    "        )\n",
    "        print(f\"  Upserted batch {i // batch_size + 1} ({len(batch)} chunks).\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    upsertion_duration = end_time - start_time\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"pinecone_upsertion_duration_seconds\", upsertion_duration)\n",
    "\n",
    "    print(f\"✅ Upsertion to Pinecone completed in {upsertion_duration:.2f} seconds.\")\n",
    "\n",
    "\n",
    "\n",
    "# upsert_to_pinecone(chunks=chunks,  embedding=embedding, index_name=index_name, batch_size= 500)  ## uncomment only when you want to insert the data in pinecode DB (i.e., if data in DB not injected before)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8657ca",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c495022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents for 'What is Cancer?' in 1.7099 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load Existing index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "# Define parameters for retrieval\n",
    "retrieval_search_type = \"similarity\"\n",
    "retrieval_k = 5\n",
    "test_query = \"What is Cancer?\"\n",
    "\n",
    "# 1. Log Retrieval Parameters\n",
    "mlflow.log_param(\"retriever_search_type\", retrieval_search_type)\n",
    "mlflow.log_param(\"retriever_k_value\", retrieval_k)\n",
    "mlflow.log_param(\"retrieval_query\", test_query) # Log the specific query used\n",
    "\n",
    "# Configure the retriever\n",
    "retriever = docsearch.as_retriever(search_type=retrieval_search_type, search_kwargs={\"k\": retrieval_k})\n",
    "\n",
    "# 2. Measure and Log Retrieval Time on Test Query\n",
    "start_time = time.time()\n",
    "retrieved_docs = retriever.invoke(test_query) ## Retriever invoked\n",
    "end_time = time.time()\n",
    "retrieval_duration = end_time - start_time\n",
    "mlflow.log_metric(\"retrieval_duration_seconds\", retrieval_duration)\n",
    "\n",
    "# 3. Log Retrieved Document Count\n",
    "num_retrieved = len(retrieved_docs)\n",
    "mlflow.log_metric(\"num_retrieved_documents\", num_retrieved)\n",
    "\n",
    "\n",
    "# 4. Log a Sample of Retrieved Documents as an Artifact\n",
    "if retrieved_docs:\n",
    "    sample_docs = []\n",
    "    # Log details of the first 3 documents as a sample\n",
    "    for i, doc in enumerate(retrieved_docs[:3]):\n",
    "        sample_docs.append({\n",
    "            \"index\": i + 1,\n",
    "            \"source\": doc.metadata.get('source', 'N/A'),\n",
    "            \"page\": doc.metadata.get('page', 'N/A'),\n",
    "            \"content_preview\": doc.page_content[:200] + \"...\" # Log first 200 chars\n",
    "        })\n",
    "\n",
    "    temp_file_path = \"retrieved_docs_sample.json\"\n",
    "    with open(temp_file_path, \"w\") as f:\n",
    "        json.dump(sample_docs, f, indent=4)\n",
    "\n",
    "    mlflow.log_artifact(temp_file_path, artifact_path=\"retrieval_output_samples\")\n",
    "    os.remove(temp_file_path) # Clean up the local temporary file\n",
    "\n",
    "print(f\"Retrieved {num_retrieved} documents for '{test_query}' in {retrieval_duration:.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9601bbc",
   "metadata": {},
   "source": [
    "## LLM Model setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2d6f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama model: llama3.1:latest...\n",
      "Ollama model 'llama3.1:latest' initialized in 0.0002 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/7jcjyd5s1cd09qhrzns_rny80000gn/T/ipykernel_67318/632503613.py:21: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  chatModel = Ollama(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama  # wrapper for local Ollama models\n",
    "\n",
    "\n",
    "# Define LLM parameters\n",
    "llm_model_name = \"llama3.1:latest\"\n",
    "llm_temperature = 0\n",
    "llm_timeout = None\n",
    "llm_max_retries = 2\n",
    "\n",
    "# Log LLM parameters\n",
    "mlflow.log_param(\"llm_provider\", \"Ollama\")\n",
    "mlflow.log_param(\"llm_class\", \"Ollama\")\n",
    "mlflow.log_param(\"llm_model_name\", llm_model_name)\n",
    "mlflow.log_param(\"llm_temperature\", llm_temperature)\n",
    "mlflow.log_param(\"llm_timeout\", llm_timeout if llm_timeout is not None else \"None\")\n",
    "mlflow.log_param(\"llm_max_retries\", llm_max_retries)\n",
    "\n",
    "\n",
    "print(f\"Initializing Ollama model: {llm_model_name}...\")\n",
    "start_time = time.time()\n",
    "chatModel = Ollama(\n",
    "    model=llm_model_name,\n",
    "    temperature=llm_temperature,\n",
    "    # Ollama wrapper doesn’t use `max_tokens` or `timeout` exactly the same way as Google GenAI,\n",
    "    # but you can still pass client kwargs if needed.\n",
    ")\n",
    "end_time = time.time()\n",
    "initialization_duration = end_time - start_time\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"llm_initialization_duration_seconds\", initialization_duration)\n",
    "\n",
    "print(f\"Ollama model '{llm_model_name}' initialized in {initialization_duration:.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e5716dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MS Dhoni, also known as Mahendra Singh Dhoni, is a former Indian international cricketer who played for the Indian national team from 2004 to 2019. He was one of the most successful and popular cricketers in the world during his playing career.\\n\\nHere are some key facts about MS Dhoni:\\n\\n**Early Life**\\n\\nDhoni was born on July 7, 1981, in Ranchi, Jharkhand (then part of Bihar), India. His father, Pan Singh Dhoni, was a railway employee, and his mother, Devki Devi, was a homemaker.\\n\\n**Cricket Career**\\n\\nDhoni started playing cricket at the age of eight and quickly rose through the ranks to become one of the most successful wicket-keepers in Indian cricket history. He made his international debut in 2004 against Bangladesh and went on to play for India in all formats of the game (Test, ODI, T20).\\n\\n**Achievements**\\n\\nDhoni's achievements are numerous:\\n\\n1. **Captaincy**: Dhoni was a successful captain of the Indian team, leading them to several victories, including the 2011 Cricket World Cup and the 2007 ICC World Twenty20.\\n2. **Wicket-keeping**: He is considered one of the greatest wicket-keepers in cricket history, known for his exceptional skills behind the stumps and his ability to take crucial catches.\\n3. **Batting**: Dhoni was a skilled batsman who could play both aggressive and defensive shots with ease. He scored over 10,000 runs in international cricket.\\n4. **Leadership**: Dhoni's leadership qualities were highly praised by his teammates and opponents alike. He was known for his calm and composed demeanor under pressure.\\n\\n**Personal Life**\\n\\nDhoni is married to Sakshi Singh Rawat, a former model and beauty pageant winner. They have two daughters, Ziva and Saanvi.\\n\\n**Post-Retirement**\\n\\nAfter announcing his retirement from international cricket in 2019, Dhoni has remained involved in the sport as the captain of Chennai Super Kings (CSK) in the Indian Premier League (IPL). He is also a successful entrepreneur and has invested in several businesses, including a restaurant chain and a sports management company.\\n\\n**Legacy**\\n\\nMS Dhoni's legacy extends beyond his on-field achievements. He is widely regarded as one of the greatest cricketers of all time, known for his humility, dedication, and leadership qualities. His impact on Indian cricket and his fans will be remembered for generations to come.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = chatModel.invoke(\"who is MS Dhoni\")\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0585b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n",
      "\n",
      "\n",
      " Prompt template defined and logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## for ready-made RAG prompts\n",
    "from langchain import hub\n",
    "# Pull a pre-made RAG prompt from LangChain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "\n",
    "## System Prompt defining\n",
    "system_prompt = (\n",
    "    \"You are an Medical assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Keep the answer concise and understandable.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create the ChatPromptTemplate\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "mlflow.log_param(\"system prompt for llm :\", system_prompt)\n",
    "print(prompt)\n",
    "print(\"\\n\\n Prompt template defined and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba084952",
   "metadata": {},
   "source": [
    "### Defining RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe1d1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the full RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chatModel\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48fb7557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The usual dosage of vitamin D for adults is 400 mg, taken two to three times a day with meals. For children under six years old, the usual dosage is 50 mg per day, divided into several small doses. The dose may be different for different people, so it's best to check with a physician or pharmacist for the correct dosage.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question using the RAG chain\n",
    "response = rag_chain.invoke(\"What dose of vitamin-D should we take?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21da96",
   "metadata": {},
   "source": [
    "## RAGAS Setup: 1.TestSet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e608f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(id: 2bdf86, type: NodeType.UNKNOWN, properties: ['page_content']),\n",
       " Node(id: 77712f, type: NodeType.UNKNOWN, properties: ['page_content'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset.graph import Node\n",
    "\n",
    "sample_nodes = [Node(\n",
    "    properties={\"page_content\": \"Einstein's theory of relativity revolutionized our understanding of space and time. It introduced the concept that time is not absolute but can change depending on the observer's frame of reference.\"}\n",
    "),Node(\n",
    "    properties={\"page_content\": \"Time dilation occurs when an object moves close to the speed of light, causing time to pass slower relative to a stationary observer. This phenomenon is a key prediction of Einstein's special theory of relativity.\"}\n",
    ")]\n",
    "sample_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b54e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
