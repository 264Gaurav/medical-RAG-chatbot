{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract text from pdf files   #load all the pdf files from data folder\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as 264Gaurav\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as 264Gaurav\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"264Gaurav/medical-chatbot\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"264Gaurav/medical-chatbot\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository 264Gaurav/medical-chatbot initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository 264Gaurav/medical-chatbot initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/28 03:21:14 INFO mlflow.tracking.fluent: Experiment with name 'PDF_Processing_Experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run ID: 89395cd4497548b782c2745f6aca6fb6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/7jcjyd5s1cd09qhrzns_rny80000gn/T/ipykernel_33631/3889639866.py:74: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  json.dump([chunk.dict() for chunk in chunks[:5]], f, indent=4) # Log first 5 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run ID: 89395cd4497548b782c2745f6aca6fb6\n",
      "Loaded 637 documents in 8.54 seconds.\n",
      "Split into 3027 chunks.\n",
      "MLflow run finished. View at https://dagshub.com/264Gaurav/medical-chatbot.mlflow\n",
      "üèÉ View run rare-pug-418 at: https://dagshub.com/264Gaurav/medical-chatbot.mlflow/#/experiments/0/runs/89395cd4497548b782c2745f6aca6fb6\n",
      "üß™ View experiment at: https://dagshub.com/264Gaurav/medical-chatbot.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# --- MLflow Integration Start ---\n",
    "import mlflow\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import dagshub\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Assume load_pdf_files is defined elsewhere, if not, here's an example:\n",
    "def load_pdf_files(data_path):\n",
    "    loader = DirectoryLoader(\n",
    "        data_path,\n",
    "        glob=\"*.pdf\", #load all the pdf files from data folder\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "# Set MLflow Tracking URI (where your MLflow server is running)\n",
    "# For local file-based tracking (default):\n",
    "# mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "# For a local server you started with 'mlflow server --host 0.0.0.0 --port 5000':\n",
    "\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "dagshub.init(repo_owner='264Gaurav', repo_name='medical-chatbot', mlflow=True)\n",
    "\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(\"PDF_Processing_Experiment\")\n",
    "\n",
    "# Start an MLflow run to log this execution\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "\n",
    "    data_dir = 'data' # This is the path to your data directory\n",
    "    mlflow.log_param(\"data_directory\", data_dir)\n",
    "    mlflow.log_param(\"pdf_loader_class\", \"PyPDFLoader\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    extracted_data = load_pdf_files(data_dir) # extracted_data will be a list of Document objects\n",
    "    end_time = time.time()\n",
    "    loading_duration = end_time - start_time\n",
    "\n",
    "    mlflow.log_metric(\"num_documents_loaded\", len(extracted_data))\n",
    "    mlflow.log_metric(\"pdf_loading_duration_seconds\", loading_duration)\n",
    "\n",
    "    # Log the number of documents loaded - CORRECTED\n",
    "    # 'extracted_data' is the list of loaded documents, so its length gives the number of documents\n",
    "    num_documents = len(extracted_data)\n",
    "    mlflow.log_param(\"num_documents\", num_documents)\n",
    "\n",
    "    # Log text splitter parameters\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 50\n",
    "    mlflow.log_param(\"chunk_size\", chunk_size)\n",
    "    mlflow.log_param(\"chunk_overlap\", chunk_overlap)\n",
    "    mlflow.log_param(\"text_splitter_class\", \"RecursiveCharacterTextSplitter\")\n",
    "\n",
    "\n",
    "    # Split the documents into chunks - CORRECTED\n",
    "    # You should split 'extracted_data' (the list of Document objects), not 'data_dir' (the string path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(extracted_data) # Use extracted_data here\n",
    "\n",
    "    # Log the number of chunks created\n",
    "    num_chunks = len(chunks)\n",
    "    mlflow.log_metric(\"num_chunks_created\", num_chunks) # Changed to metric as it's an output count\n",
    "\n",
    "    # Optional: Log a sample of the first few chunks as an artifact\n",
    "    if chunks:\n",
    "        chunks_sample_path = \"chunks_sample.json\"\n",
    "        with open(chunks_sample_path, \"w\") as f:\n",
    "            json.dump([chunk.dict() for chunk in chunks[:5]], f, indent=4) # Log first 5 chunks\n",
    "        mlflow.log_artifact(chunks_sample_path, artifact_path=\"data_processing_artifacts\")\n",
    "        os.remove(chunks_sample_path) # Clean up local file\n",
    "\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "    print(f\"Loaded {num_documents} documents in {loading_duration:.2f} seconds.\")\n",
    "    print(f\"Split into {num_chunks} chunks.\")\n",
    "    print(f\"MLflow run finished. View at {mlflow.get_tracking_uri()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:00:02-05:00', 'moddate': '2004-12-18T16:15:31-06:00', 'source': 'data/Medical_book.pdf', 'total_pages': 637, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:00:02-05:00', 'moddate': '2004-12-18T16:15:31-06:00', 'source': 'data/Medical_book.pdf', 'total_pages': 637, 'page': 1, 'page_label': '2'}, page_content='The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:00:02-05:00', 'moddate': '2004-12-18T16:15:31-06:00', 'source': 'data/Medical_book.pdf', 'total_pages': 637, 'page': 2, 'page_label': '3'}, page_content='The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nJACQUELINE L. LONGE, EDITOR\\nDEIRDRE S. BLANCHFIELD, ASSOCIATE EDITOR\\nVOLUME\\nA-B\\n1'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:00:02-05:00', 'moddate': '2004-12-18T16:15:31-06:00', 'source': 'data/Medical_book.pdf', 'total_pages': 637, 'page': 3, 'page_label': '4'}, page_content='STAFF\\nJacqueline L. Longe, Project Editor\\nDeirdre S. Blanchfield, Associate Editor\\nChristine B. Jeryan, Managing Editor\\nDonna Olendorf, Senior Editor\\nStacey Blachford, Associate Editor\\nKate Kretschmann, Melissa C. McDade, Ryan\\nThomason, Assistant Editors\\nMark Springer, Technical Specialist\\nAndrea Lopeman, Programmer/Analyst\\nBarbara J. Yarrow,Manager, Imaging and Multimedia\\nContent\\nRobyn V . Young,Project Manager, Imaging and\\nMultimedia Content\\nDean Dauphinais, Senior Editor, Imaging and\\nMultimedia Content\\nKelly A. Quin, Editor, Imaging and Multimedia Content\\nLeitha Etheridge-Sims, Mary K. Grimes, Dave Oblender,\\nImage Catalogers\\nPamela A. Reed, Imaging Coordinator\\nRandy Bassett, Imaging Supervisor\\nRobert Duncan, Senior Imaging Specialist\\nDan Newell, Imaging Specialist\\nChristine O‚ÄôBryan,Graphic Specialist\\nMaria Franklin, Permissions Manager\\nMargaret A. Chamberlain, Permissions Specialist\\nMichelle DiMercurio, Senior Art Director\\nMike Logusz, Graphic Artist\\nMary Beth Trimper,Manager, Composition and\\nElectronic Prepress\\nEvi Seoud, Assistant Manager, Composition Purchasing\\nand Electronic Prepress\\nDorothy Maki, Manufacturing Manager\\nWendy Blurton, Senior Manufacturing Specialist\\nThe GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nSince this page cannot legibly accommodate all copyright notices, the\\nacknowledgments constitute an extension of the copyright notice.\\nWhile every effort has been made to ensure the reliability of the infor-\\nmation presented in this publication, the Gale Group neither guarantees\\nthe accuracy of the data contained herein nor assumes any responsibili-\\nty for errors, omissions or discrepancies. The Gale Group accepts no\\npayment for listing, and inclusion in the publication of any organiza-\\ntion, agency, institution, publication, service, or individual does not\\nimply endorsement of the editor or publisher. Errors brought to the\\nattention of the publisher and verified to the satisfaction of the publish-\\ner will be corrected in future editions.\\nThis book is printed on recycled paper that meets Environmental Pro-\\ntection Agency standards.\\nThe paper used in this publication meets the minimum requirements of\\nAmerican National Standard for Information Sciences-Permanence\\nPaper for Printed Library Materials, ANSI Z39.48-1984.\\nThis publication is a creative work fully protected by all applicable\\ncopyright laws, as well as by misappropriation, trade secret, unfair com-\\npetition, and other applicable laws. The authors and editor of this work\\nhave added value to the underlying factual material herein through one\\nor more of the following: unique and original selection, coordination,\\nexpression, arrangement, and classification of the information.\\nGale Group and design is a trademark used herein under license.\\nAll rights to this publication will be vigorously defended.\\nCopyright ¬© 2002\\nGale Group\\n27500 Drake Road\\nFarmington Hills, MI 48331-3535\\nAll rights reserved including the right of reproduction in whole or in\\npart in any form.\\nISBN 0-7876-5489-2 (set)\\n0-7876-5490-6 (V ol. 1)\\n0-7876-5491-4 (V ol. 2)\\n0-7876-5492-2 (V ol. 3)\\n0-7876-5493-0 (V ol. 4)\\n0-7876-5494-9 (V ol. 5)\\nPrinted in the United States of America\\n10 9 8 7 6 5 4 3 2 1\\nLibrary of Congress Cataloging-in-Publication Data\\nGale encyclopedia of medicine / Jacqueline L. Longe, editor;\\nDeirdre S. Blanchfield, associate editor ‚Äî 2nd ed.\\np. cm.\\nIncludes bibliographical references and index.\\nContents: V ol. 1. A-B ‚Äî v. 2. C-F ‚Äî v. 3.\\nG-M ‚Äî v. 4. N-S ‚Äî v. 5. T-Z.\\nISBN 0-7876-5489-2 (set: hardcover) ‚Äî ISBN 0-7876-5490-6\\n(vol. 1) ‚Äî ISBN 0-7876-5491-4 (vol. 2) ‚Äî ISBN 0-7876-5492-2\\n(vol. 3) ‚Äî ISBN 0-7876-5493-0 (vol. 4) ‚Äî ISBN 0-7876-5494-9\\n(vol. 5)\\n1. Internal medicine‚ÄîEncyclopedias. I. Longe, Jacqueline L. \\nII. Blanchfield, Deirdre S. III. Gale Research Company.\\nRC41.G35 2001\\n616‚Äô.003‚Äîdc21\\n2001051245'),\n",
       " Document(metadata={'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'creator': 'PyPDF', 'creationdate': '2004-12-18T17:00:02-05:00', 'moddate': '2004-12-18T16:15:31-06:00', 'source': 'data/Medical_book.pdf', 'total_pages': 637, 'page': 4, 'page_label': '5'}, page_content='Introduction.................................................... ix\\nAdvisory Board.............................................. xi\\nContributors ................................................. xiii\\nEntries\\nVolume 1: A-B.............................................. 1\\nVolume 2: C-F.......................................... 625\\nVolume 3: G-M....................................... 1375\\nVolume 4: N-S........................................ 2307\\nVolume 5: T-Z........................................ 3237\\nOrganizations ............................................ 3603\\nGeneral Index............................................ 3625\\nGALE ENCYCLOPEDIA OF MEDICINE 2 V\\nCONTENTS')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data[:5] #show first 5 pages extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "# from langchain.schema import Document\n",
    "\n",
    "# ##DATA cleaning and filtering\n",
    "# def filter_docs(docs: List[Document]) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     Given a list of Document objects, filter out those with new list of Document objects\n",
    "#     containing only 'source' in metadata and the original page_content.\n",
    "#     \"\"\"\n",
    "#     minimal_docs: List[Document] = []\n",
    "#     for doc in docs:\n",
    "#         src=doc.metadata.get('source')\n",
    "#         minimal_docs.append(\n",
    "#             Document(\n",
    "#                 page_content=doc.page_content,\n",
    "#                 metadata={\"source\": src}\n",
    "#             )\n",
    "#         )\n",
    "#     return minimal_docs\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "##DATA cleaning and filtering\n",
    "def filter_docs(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, filter out those with new list of Document objects\n",
    "    containing only 'source' in metadata and the original page_content.\n",
    "\n",
    "    MLflow Tracking:\n",
    "    - Logs 'input_documents_count' as a metric.\n",
    "    - Logs 'output_documents_count' as a metric.\n",
    "    - Logs 'documents_filtered_count' as a metric.\n",
    "    - Logs 'filter_function_name' as a parameter.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Log initial state\n",
    "    input_documents_count = len(docs)\n",
    "    mlflow.log_metric(\"input_documents_count_for_filtering\", input_documents_count)\n",
    "    mlflow.log_param(\"filter_function_name\", \"filter_docs\")\n",
    "\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get('source')\n",
    "        # Here, you might add more complex filtering logic if needed\n",
    "        # For this specific function, it always adds the document, just with minimal metadata\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    output_documents_count = len(minimal_docs)\n",
    "    documents_filtered_count = input_documents_count - output_documents_count # Will be 0 if all are kept\n",
    "\n",
    "    # Log metrics after processing\n",
    "    mlflow.log_metric(\"output_documents_count_after_filtering\", output_documents_count)\n",
    "    mlflow.log_metric(\"documents_filtered_count\", documents_filtered_count)\n",
    "\n",
    "    # Optional: Log a sample of filtered documents as an artifact\n",
    "    if minimal_docs:\n",
    "        sample_docs_path = \"filtered_docs_sample.json\"\n",
    "        # Log metadata from first few documents as a sample\n",
    "        sample_data = []\n",
    "        for i, doc in enumerate(minimal_docs[:5]): # Log first 5 documents\n",
    "            sample_data.append({\n",
    "                \"page_content_preview\": doc.page_content[:200] + \"...\",\n",
    "                \"metadata\": doc.metadata\n",
    "            })\n",
    "        with open(sample_docs_path, \"w\") as f:\n",
    "            json.dump(sample_data, f, indent=4)\n",
    "        mlflow.log_artifact(sample_docs_path, artifact_path=\"data_filtering_artifacts\")\n",
    "        os.remove(sample_docs_path) # Clean up local file\n",
    "\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/Medical_book.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nJACQUELINE L. LONGE, EDITOR\\nDEIRDRE S. BLANCHFIELD, ASSOCIATE EDITOR\\nVOLUME\\nA-B\\n1'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='STAFF\\nJacqueline L. Longe, Project Editor\\nDeirdre S. Blanchfield, Associate Editor\\nChristine B. Jeryan, Managing Editor\\nDonna Olendorf, Senior Editor\\nStacey Blachford, Associate Editor\\nKate Kretschmann, Melissa C. McDade, Ryan\\nThomason, Assistant Editors\\nMark Springer, Technical Specialist\\nAndrea Lopeman, Programmer/Analyst\\nBarbara J. Yarrow,Manager, Imaging and Multimedia\\nContent\\nRobyn V . Young,Project Manager, Imaging and\\nMultimedia Content\\nDean Dauphinais, Senior Editor, Imaging and\\nMultimedia Content\\nKelly A. Quin, Editor, Imaging and Multimedia Content\\nLeitha Etheridge-Sims, Mary K. Grimes, Dave Oblender,\\nImage Catalogers\\nPamela A. Reed, Imaging Coordinator\\nRandy Bassett, Imaging Supervisor\\nRobert Duncan, Senior Imaging Specialist\\nDan Newell, Imaging Specialist\\nChristine O‚ÄôBryan,Graphic Specialist\\nMaria Franklin, Permissions Manager\\nMargaret A. Chamberlain, Permissions Specialist\\nMichelle DiMercurio, Senior Art Director\\nMike Logusz, Graphic Artist\\nMary Beth Trimper,Manager, Composition and\\nElectronic Prepress\\nEvi Seoud, Assistant Manager, Composition Purchasing\\nand Electronic Prepress\\nDorothy Maki, Manufacturing Manager\\nWendy Blurton, Senior Manufacturing Specialist\\nThe GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nSince this page cannot legibly accommodate all copyright notices, the\\nacknowledgments constitute an extension of the copyright notice.\\nWhile every effort has been made to ensure the reliability of the infor-\\nmation presented in this publication, the Gale Group neither guarantees\\nthe accuracy of the data contained herein nor assumes any responsibili-\\nty for errors, omissions or discrepancies. The Gale Group accepts no\\npayment for listing, and inclusion in the publication of any organiza-\\ntion, agency, institution, publication, service, or individual does not\\nimply endorsement of the editor or publisher. Errors brought to the\\nattention of the publisher and verified to the satisfaction of the publish-\\ner will be corrected in future editions.\\nThis book is printed on recycled paper that meets Environmental Pro-\\ntection Agency standards.\\nThe paper used in this publication meets the minimum requirements of\\nAmerican National Standard for Information Sciences-Permanence\\nPaper for Printed Library Materials, ANSI Z39.48-1984.\\nThis publication is a creative work fully protected by all applicable\\ncopyright laws, as well as by misappropriation, trade secret, unfair com-\\npetition, and other applicable laws. The authors and editor of this work\\nhave added value to the underlying factual material herein through one\\nor more of the following: unique and original selection, coordination,\\nexpression, arrangement, and classification of the information.\\nGale Group and design is a trademark used herein under license.\\nAll rights to this publication will be vigorously defended.\\nCopyright ¬© 2002\\nGale Group\\n27500 Drake Road\\nFarmington Hills, MI 48331-3535\\nAll rights reserved including the right of reproduction in whole or in\\npart in any form.\\nISBN 0-7876-5489-2 (set)\\n0-7876-5490-6 (V ol. 1)\\n0-7876-5491-4 (V ol. 2)\\n0-7876-5492-2 (V ol. 3)\\n0-7876-5493-0 (V ol. 4)\\n0-7876-5494-9 (V ol. 5)\\nPrinted in the United States of America\\n10 9 8 7 6 5 4 3 2 1\\nLibrary of Congress Cataloging-in-Publication Data\\nGale encyclopedia of medicine / Jacqueline L. Longe, editor;\\nDeirdre S. Blanchfield, associate editor ‚Äî 2nd ed.\\np. cm.\\nIncludes bibliographical references and index.\\nContents: V ol. 1. A-B ‚Äî v. 2. C-F ‚Äî v. 3.\\nG-M ‚Äî v. 4. N-S ‚Äî v. 5. T-Z.\\nISBN 0-7876-5489-2 (set: hardcover) ‚Äî ISBN 0-7876-5490-6\\n(vol. 1) ‚Äî ISBN 0-7876-5491-4 (vol. 2) ‚Äî ISBN 0-7876-5492-2\\n(vol. 3) ‚Äî ISBN 0-7876-5493-0 (vol. 4) ‚Äî ISBN 0-7876-5494-9\\n(vol. 5)\\n1. Internal medicine‚ÄîEncyclopedias. I. Longe, Jacqueline L. \\nII. Blanchfield, Deirdre S. III. Gale Research Company.\\nRC41.G35 2001\\n616‚Äô.003‚Äîdc21\\n2001051245'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='Introduction.................................................... ix\\nAdvisory Board.............................................. xi\\nContributors ................................................. xiii\\nEntries\\nVolume 1: A-B.............................................. 1\\nVolume 2: C-F.......................................... 625\\nVolume 3: G-M....................................... 1375\\nVolume 4: N-S........................................ 2307\\nVolume 5: T-Z........................................ 3237\\nOrganizations ............................................ 3603\\nGeneral Index............................................ 3625\\nGALE ENCYCLOPEDIA OF MEDICINE 2 V\\nCONTENTS')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimal_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(minimal_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 3027\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(minimal_docs)\n",
    "print(f\"Number of text chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='The GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nJACQUELINE L. LONGE, EDITOR\\nDEIRDRE S. BLANCHFIELD, ASSOCIATE EDITOR\\nVOLUME\\nA-B\\n1'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='STAFF\\nJacqueline L. Longe, Project Editor\\nDeirdre S. Blanchfield, Associate Editor\\nChristine B. Jeryan, Managing Editor\\nDonna Olendorf, Senior Editor\\nStacey Blachford, Associate Editor\\nKate Kretschmann, Melissa C. McDade, Ryan\\nThomason, Assistant Editors\\nMark Springer, Technical Specialist\\nAndrea Lopeman, Programmer/Analyst\\nBarbara J. Yarrow,Manager, Imaging and Multimedia\\nContent\\nRobyn V . Young,Project Manager, Imaging and\\nMultimedia Content\\nDean Dauphinais, Senior Editor, Imaging and\\nMultimedia Content\\nKelly A. Quin, Editor, Imaging and Multimedia Content\\nLeitha Etheridge-Sims, Mary K. Grimes, Dave Oblender,\\nImage Catalogers\\nPamela A. Reed, Imaging Coordinator\\nRandy Bassett, Imaging Supervisor\\nRobert Duncan, Senior Imaging Specialist\\nDan Newell, Imaging Specialist\\nChristine O‚ÄôBryan,Graphic Specialist\\nMaria Franklin, Permissions Manager\\nMargaret A. Chamberlain, Permissions Specialist\\nMichelle DiMercurio, Senior Art Director\\nMike Logusz, Graphic Artist'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='Mike Logusz, Graphic Artist\\nMary Beth Trimper,Manager, Composition and\\nElectronic Prepress\\nEvi Seoud, Assistant Manager, Composition Purchasing\\nand Electronic Prepress\\nDorothy Maki, Manufacturing Manager\\nWendy Blurton, Senior Manufacturing Specialist\\nThe GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nSince this page cannot legibly accommodate all copyright notices, the\\nacknowledgments constitute an extension of the copyright notice.\\nWhile every effort has been made to ensure the reliability of the infor-\\nmation presented in this publication, the Gale Group neither guarantees\\nthe accuracy of the data contained herein nor assumes any responsibili-\\nty for errors, omissions or discrepancies. The Gale Group accepts no\\npayment for listing, and inclusion in the publication of any organiza-\\ntion, agency, institution, publication, service, or individual does not\\nimply endorsement of the editor or publisher. Errors brought to the'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf'}, page_content='attention of the publisher and verified to the satisfaction of the publish-\\ner will be corrected in future editions.\\nThis book is printed on recycled paper that meets Environmental Pro-\\ntection Agency standards.\\nThe paper used in this publication meets the minimum requirements of\\nAmerican National Standard for Information Sciences-Permanence\\nPaper for Printed Library Materials, ANSI Z39.48-1984.\\nThis publication is a creative work fully protected by all applicable\\ncopyright laws, as well as by misappropriation, trade secret, unfair com-\\npetition, and other applicable laws. The authors and editor of this work\\nhave added value to the underlying factual material herein through one\\nor more of the following: unique and original selection, coordination,\\nexpression, arrangement, and classification of the information.\\nGale Group and design is a trademark used herein under license.\\nAll rights to this publication will be vigorously defended.\\nCopyright ¬© 2002\\nGale Group\\n27500 Drake Road')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[:5]  # show first 5 text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/7jcjyd5s1cd09qhrzns_rny80000gn/T/ipykernel_33631/3581463811.py:16: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n",
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model 'sentence-transformers/all-MiniLM-L6-v2' in 6.37 seconds.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "     # Log embedding model parameters\n",
    "    mlflow.log_param(\"embedding_model_name\", model_name)\n",
    "    mlflow.log_param(\"embedding_provider\", \"HuggingFace\")\n",
    "    mlflow.log_param(\"embedding_class\", \"HuggingFaceBgeEmbeddings\")\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    embeddings = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    loading_duration = end_time - start_time\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"embedding_loading_duration_seconds\", loading_duration)\n",
    "\n",
    "    print(f\"Loaded embedding model '{model_name}' in {loading_duration:.2f} seconds.\")\n",
    "\n",
    "\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.046040285378694534,\n",
       " 0.05801551416516304,\n",
       " 0.025460267439484596,\n",
       " -0.018886055797338486,\n",
       " -0.056887563318014145,\n",
       " -0.08692731708288193,\n",
       " 0.0589798241853714,\n",
       " 0.11981295794248581,\n",
       " 0.02426879107952118,\n",
       " -0.05556875094771385,\n",
       " -0.018091533333063126,\n",
       " 0.016502052545547485,\n",
       " -0.02637430652976036,\n",
       " 0.011251488700509071,\n",
       " 0.07773943245410919,\n",
       " -0.020143186673521996,\n",
       " 0.03203606978058815,\n",
       " -0.0955556184053421,\n",
       " -0.016210833564400673,\n",
       " 0.05219794809818268,\n",
       " 0.08739541471004486,\n",
       " 0.13612855970859528,\n",
       " 0.054607000201940536,\n",
       " -0.007297117728739977,\n",
       " -0.03621230646967888,\n",
       " 0.0024328413419425488,\n",
       " 0.003911045845597982,\n",
       " -0.029571056365966797,\n",
       " 0.07670431584119797,\n",
       " 0.025621410459280014,\n",
       " 0.0118556534871459,\n",
       " -0.010180271230638027,\n",
       " 0.07361406832933426,\n",
       " 0.07227780669927597,\n",
       " 0.030669499188661575,\n",
       " 0.0720134899020195,\n",
       " -0.042889174073934555,\n",
       " -0.0035438723862171173,\n",
       " -0.010086719878017902,\n",
       " 0.02317650057375431,\n",
       " -0.01849573850631714,\n",
       " -0.07412372529506683,\n",
       " -0.009275458753108978,\n",
       " -0.010359229519963264,\n",
       " 0.04237191379070282,\n",
       " -0.005520131438970566,\n",
       " -0.0885910615324974,\n",
       " 0.036863721907138824,\n",
       " 0.08046330511569977,\n",
       " 0.035077016800642014,\n",
       " -0.15504509210586548,\n",
       " -0.02753676287829876,\n",
       " 0.017749490216374397,\n",
       " 0.13738586008548737,\n",
       " -0.019703075289726257,\n",
       " 0.015997014939785004,\n",
       " -0.11001671850681305,\n",
       " 0.009737587533891201,\n",
       " -0.013350900262594223,\n",
       " -0.07568055391311646,\n",
       " -0.07244735956192017,\n",
       " 0.04905378073453903,\n",
       " 0.04273896664381027,\n",
       " 0.04207010567188263,\n",
       " 0.012171339243650436,\n",
       " -0.027571966871619225,\n",
       " -0.007979040965437889,\n",
       " -0.037267666310071945,\n",
       " 0.040096960961818695,\n",
       " 0.002667341148480773,\n",
       " -0.0559057854115963,\n",
       " -0.04491705447435379,\n",
       " -0.02665581740438938,\n",
       " 0.1398857831954956,\n",
       " -0.04285690560936928,\n",
       " -0.015370435081422329,\n",
       " 0.06271129846572876,\n",
       " -0.05149133875966072,\n",
       " 0.01825842633843422,\n",
       " 0.007777387276291847,\n",
       " -0.008591875433921814,\n",
       " 0.00029139939579181373,\n",
       " 0.05267021059989929,\n",
       " 0.047161851078271866,\n",
       " -0.07296288758516312,\n",
       " 0.0358746200799942,\n",
       " 0.06014901027083397,\n",
       " 0.03073233552277088,\n",
       " -0.037954431027173996,\n",
       " 0.0006853782688267529,\n",
       " -0.05629456415772438,\n",
       " -0.07901748269796371,\n",
       " 0.04951607808470726,\n",
       " 0.02648390270769596,\n",
       " -0.020193323493003845,\n",
       " -0.035322051495313644,\n",
       " 0.0015370473265647888,\n",
       " 0.006583911366760731,\n",
       " -0.0017520291730761528,\n",
       " 0.039331771433353424,\n",
       " 0.015913501381874084,\n",
       " -0.01875833235681057,\n",
       " -0.025687580928206444,\n",
       " -0.022663449868559837,\n",
       " -0.04057736322283745,\n",
       " -0.011724560521543026,\n",
       " -0.028763024136424065,\n",
       " -0.009930739179253578,\n",
       " 0.04581870883703232,\n",
       " -0.04008575156331062,\n",
       " -0.053712110966444016,\n",
       " -0.02761111781001091,\n",
       " 0.037005700170993805,\n",
       " -0.04585383087396622,\n",
       " 0.06729719787836075,\n",
       " 0.0361991785466671,\n",
       " -0.02746512182056904,\n",
       " 0.02884499542415142,\n",
       " 0.0627012699842453,\n",
       " 0.063658706843853,\n",
       " 0.049780987203121185,\n",
       " -0.07197543978691101,\n",
       " -0.06332267075777054,\n",
       " -0.047965239733457565,\n",
       " 0.037326958030462265,\n",
       " -0.012968971394002438,\n",
       " -0.00558760529384017,\n",
       " -2.0498059267997886e-33,\n",
       " 0.060849301517009735,\n",
       " -0.025387033820152283,\n",
       " 0.06411628425121307,\n",
       " 0.08680006861686707,\n",
       " -0.016316045075654984,\n",
       " 0.0870196521282196,\n",
       " -0.04188378527760506,\n",
       " -0.02926689386367798,\n",
       " 0.036396272480487823,\n",
       " -0.026044759899377823,\n",
       " -0.06383315473794937,\n",
       " 0.009505572728812695,\n",
       " 0.0033690938726067543,\n",
       " 0.0169342290610075,\n",
       " -0.06610273569822311,\n",
       " -0.05794598534703255,\n",
       " -0.0527755506336689,\n",
       " 0.08605906367301941,\n",
       " 0.001887044170871377,\n",
       " 0.05726400390267372,\n",
       " -0.002323899883776903,\n",
       " -0.04025440663099289,\n",
       " -0.0014745064545422792,\n",
       " 0.13552330434322357,\n",
       " 0.011082485318183899,\n",
       " 0.09722161293029785,\n",
       " -0.0531403124332428,\n",
       " -0.090126633644104,\n",
       " 0.08863223344087601,\n",
       " -0.0016135191544890404,\n",
       " -0.08203680068254471,\n",
       " 0.012853238731622696,\n",
       " -0.03642663359642029,\n",
       " -0.009922674857079983,\n",
       " 0.0252787284553051,\n",
       " 0.08106934279203415,\n",
       " -0.015423453412950039,\n",
       " -0.029315168038010597,\n",
       " -0.06909731030464172,\n",
       " 0.009698008187115192,\n",
       " -0.06109946221113205,\n",
       " 0.0001974022015929222,\n",
       " 0.02605268731713295,\n",
       " -0.04472070559859276,\n",
       " 0.061791107058525085,\n",
       " -0.028017744421958923,\n",
       " -0.1136859729886055,\n",
       " -0.060434456914663315,\n",
       " -0.01681932806968689,\n",
       " -0.026370560750365257,\n",
       " -0.06361355632543564,\n",
       " 0.07598043233156204,\n",
       " -0.0012628859840333462,\n",
       " -0.02466101571917534,\n",
       " 0.019695866852998734,\n",
       " -0.044788382947444916,\n",
       " -0.026830650866031647,\n",
       " 0.060293372720479965,\n",
       " -0.03276306390762329,\n",
       " 0.019626351073384285,\n",
       " 0.011598770506680012,\n",
       " 0.028325706720352173,\n",
       " 0.02244669385254383,\n",
       " 0.044603098183870316,\n",
       " 0.026852821931242943,\n",
       " -0.08057761937379837,\n",
       " 0.012883295305073261,\n",
       " -0.09555833041667938,\n",
       " 0.03828984126448631,\n",
       " -0.013984580524265766,\n",
       " -0.04326591640710831,\n",
       " 0.06349878758192062,\n",
       " -0.03677935153245926,\n",
       " 0.03318866342306137,\n",
       " -0.10167078673839569,\n",
       " 0.043840471655130386,\n",
       " -0.009237462654709816,\n",
       " -0.04344042018055916,\n",
       " 0.03802490234375,\n",
       " -0.05167973041534424,\n",
       " -0.012357037514448166,\n",
       " -0.01997452788054943,\n",
       " -0.035114604979753494,\n",
       " 0.022388650104403496,\n",
       " 0.06007252261042595,\n",
       " -0.08775407075881958,\n",
       " -0.028740236535668373,\n",
       " -0.045243144035339355,\n",
       " -0.07600881904363632,\n",
       " 0.03215544670820236,\n",
       " -0.05630987510085106,\n",
       " 0.057341985404491425,\n",
       " -0.04490956664085388,\n",
       " 0.01340755820274353,\n",
       " -0.04947086423635483,\n",
       " -6.70331221083364e-35,\n",
       " 0.05321205407381058,\n",
       " 0.03789461404085159,\n",
       " -0.04746445640921593,\n",
       " 0.05392041057348251,\n",
       " 0.03225989267230034,\n",
       " 0.015489809215068817,\n",
       " 0.038742393255233765,\n",
       " 0.05942991375923157,\n",
       " 0.028278492391109467,\n",
       " 0.06122833862900734,\n",
       " -0.005670581012964249,\n",
       " -0.021378200501203537,\n",
       " 0.04193510860204697,\n",
       " -0.01326312031596899,\n",
       " -0.028656020760536194,\n",
       " 0.0915563777089119,\n",
       " 0.012904781848192215,\n",
       " -0.0432293638586998,\n",
       " -0.04672420769929886,\n",
       " 0.01789509877562523,\n",
       " -0.052758000791072845,\n",
       " 0.11218613386154175,\n",
       " -0.08948920667171478,\n",
       " 0.0352519154548645,\n",
       " 0.010656353086233139,\n",
       " 0.040460940450429916,\n",
       " 0.008940923027694225,\n",
       " 0.004023861140012741,\n",
       " -0.0612625814974308,\n",
       " -0.06673664599657059,\n",
       " -0.00728880288079381,\n",
       " 0.023500941693782806,\n",
       " -0.06491667777299881,\n",
       " 0.00882895290851593,\n",
       " -0.00491040525957942,\n",
       " 0.002207196783274412,\n",
       " 0.08114876598119736,\n",
       " -0.007971135899424553,\n",
       " 0.009370659478008747,\n",
       " -0.10152675211429596,\n",
       " 0.13292060792446136,\n",
       " 0.03345365449786186,\n",
       " -0.04307826608419418,\n",
       " 0.014762649312615395,\n",
       " 0.017084350809454918,\n",
       " -0.05862581357359886,\n",
       " -0.08730672299861908,\n",
       " -0.036653440445661545,\n",
       " -0.004974869545549154,\n",
       " 0.031482014805078506,\n",
       " -0.009111980907619,\n",
       " -0.06599478423595428,\n",
       " 0.056094519793987274,\n",
       " -0.09904327243566513,\n",
       " -0.0826999843120575,\n",
       " -0.04131360724568367,\n",
       " -0.028987081721425056,\n",
       " -0.0544731579720974,\n",
       " -0.018562721088528633,\n",
       " -0.043992459774017334,\n",
       " -0.002326485700905323,\n",
       " -0.04790820553898811,\n",
       " 0.022588493302464485,\n",
       " 0.04049306735396385,\n",
       " 0.011999541893601418,\n",
       " -0.018167901784181595,\n",
       " 0.04657707363367081,\n",
       " 0.09851590543985367,\n",
       " 0.03598108887672424,\n",
       " -0.07587966322898865,\n",
       " 0.12553586065769196,\n",
       " -0.06581901758909225,\n",
       " 0.025578556582331657,\n",
       " 0.050880543887615204,\n",
       " 0.05710052326321602,\n",
       " -0.0026653760578483343,\n",
       " -0.008800726383924484,\n",
       " -0.05708124861121178,\n",
       " -0.03767872974276543,\n",
       " -0.0150039317086339,\n",
       " -0.012114820070564747,\n",
       " -0.034557100385427475,\n",
       " 0.04893592745065689,\n",
       " 0.08927004039287567,\n",
       " -0.014709661714732647,\n",
       " 0.010780318640172482,\n",
       " 0.03841659426689148,\n",
       " 0.02291504479944706,\n",
       " -0.04745316877961159,\n",
       " 0.016269614920020103,\n",
       " -0.06347376108169556,\n",
       " 0.01628907211124897,\n",
       " -0.04249721020460129,\n",
       " 0.02601679600775242,\n",
       " -0.0035353873390704393,\n",
       " -2.209682570253335e-08,\n",
       " -0.006068290211260319,\n",
       " -0.06232931837439537,\n",
       " 0.012646554037928581,\n",
       " 0.053608473390340805,\n",
       " 0.020183835178613663,\n",
       " -0.012016519904136658,\n",
       " -0.059888340532779694,\n",
       " 0.008176485076546669,\n",
       " -0.03607648238539696,\n",
       " -0.002801349852234125,\n",
       " -0.010618842206895351,\n",
       " 0.056156545877456665,\n",
       " 0.01683952286839485,\n",
       " -0.0206744447350502,\n",
       " 0.1434876024723053,\n",
       " 0.0577508807182312,\n",
       " -0.03759347274899483,\n",
       " -0.003135829698294401,\n",
       " -0.04834383726119995,\n",
       " -0.09391053020954132,\n",
       " 0.04749032109975815,\n",
       " -0.035105060786008835,\n",
       " -0.019085027277469635,\n",
       " 0.06781116127967834,\n",
       " -0.04009055718779564,\n",
       " 0.008164986968040466,\n",
       " -0.010601332411170006,\n",
       " 0.07451271265745163,\n",
       " -0.09201494604349136,\n",
       " -0.004512979183346033,\n",
       " -0.09137729555368423,\n",
       " 0.038762517273426056,\n",
       " -0.002766313962638378,\n",
       " -0.004711485933512449,\n",
       " 0.011985240504145622,\n",
       " -0.05984220653772354,\n",
       " -0.018039828166365623,\n",
       " -0.10278434306383133,\n",
       " 0.013449295423924923,\n",
       " 0.01663994789123535,\n",
       " 0.046356603503227234,\n",
       " 0.029019111767411232,\n",
       " 0.02031647227704525,\n",
       " -0.053018298000097275,\n",
       " 0.028827711939811707,\n",
       " -0.06696503609418869,\n",
       " 0.028058839961886406,\n",
       " -0.061869096010923386,\n",
       " -0.056534815579652786,\n",
       " -0.07942584156990051,\n",
       " -0.02692568302154541,\n",
       " 0.018671803176403046,\n",
       " 0.06261853128671646,\n",
       " 0.031475216150283813,\n",
       " -0.0024856985546648502,\n",
       " 0.0625108852982521,\n",
       " 0.036848943680524826,\n",
       " 0.006971747614443302,\n",
       " -0.036771319806575775,\n",
       " 0.008701927959918976,\n",
       " 0.06435220688581467,\n",
       " 0.15905961394309998,\n",
       " 0.056322261691093445,\n",
       " -0.014336234889924526]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embedding.embed_query(\"Hello to medical chatbot.\")\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Vector length: {len(vector)}\")  # Check the length of the vector  ## Dimentions of the vector\n",
    "mlflow.log_param(\"vector_embedding_size\", len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pinecone is the leading vector database for building accurate and performant AI applications\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pinecone is the leading vector database for building accurate and performant AI applications\n",
    "from pinecone import Pinecone\n",
    "pinecone_api = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x169c93c20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone index 'medical-chatbot' already exists. Connecting...\n",
      "Connected to Pinecone index 'medical-chatbot' in 0.9872 seconds.\n"
     ]
    }
   ],
   "source": [
    "# from pinecone import ServerlessSpec\n",
    "\n",
    "# index_name = 'medical-chatbot'\n",
    "\n",
    "# if not pc.has_index(index_name):\n",
    "#     pc.create_index(\n",
    "#         name=index_name,\n",
    "#         dimension=384,  # Dimension of the embeddings\n",
    "#         metric='cosine',  # Similarity metric - cosine similarity\n",
    "#         spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "#     )\n",
    "\n",
    "# index = pc.Index(index_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "\n",
    "\n",
    "# Log stage name\n",
    "mlflow.log_param(\"stage\", \"pinecone_index_setup\")\n",
    "\n",
    "index_name = 'medical-chatbot'\n",
    "dimension = 384  # Dimension of the embeddings (should match your embedding model output)\n",
    "metric = 'cosine' # Similarity metric\n",
    "\n",
    "# Log Pinecone index parameters\n",
    "mlflow.log_param(\"pinecone_index_name\", index_name)\n",
    "mlflow.log_param(\"pinecone_index_dimension\", dimension)\n",
    "mlflow.log_param(\"pinecone_index_metric\", metric)\n",
    "\n",
    "# Define the serverless spec parameters\n",
    "cloud_provider = \"aws\"\n",
    "region = \"us-east-1\"\n",
    "mlflow.log_param(\"pinecone_cloud_provider\", cloud_provider)\n",
    "mlflow.log_param(\"pinecone_region\", region)\n",
    "\n",
    "\n",
    "# Check if index exists and create if not\n",
    "start_check_time = time.time()\n",
    "index_exists = pc.has_index(index_name)\n",
    "end_check_time = time.time()\n",
    "check_duration = end_check_time - start_check_time\n",
    "\n",
    "mlflow.log_metric(\"pinecone_index_exists_check_duration_seconds\", check_duration)\n",
    "mlflow.log_param(\"pinecone_index_existed_before_run\", index_exists)\n",
    "\n",
    "if not index_exists:\n",
    "    mlflow.log_param(\"pinecone_index_action\", \"created_new_index\")\n",
    "    print(f\"Pinecone index '{index_name}' does not exist. Creating...\")\n",
    "    start_create_time = time.time()\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=metric,\n",
    "        spec=ServerlessSpec(cloud=cloud_provider, region=region)\n",
    "    )\n",
    "    end_create_time = time.time()\n",
    "    creation_duration = end_create_time - start_create_time\n",
    "    mlflow.log_metric(\"pinecone_index_creation_duration_seconds\", creation_duration)\n",
    "    print(f\"Pinecone index '{index_name}' created in {creation_duration:.2f} seconds.\")\n",
    "else:\n",
    "    mlflow.log_param(\"pinecone_index_action\", \"connected_to_existing_index\")\n",
    "    print(f\"Pinecone index '{index_name}' already exists. Connecting...\")\n",
    "    mlflow.log_metric(\"pinecone_index_creation_duration_seconds\", 0) # Log 0 if not created\n",
    "\n",
    "# Connect to the index\n",
    "start_connect_time = time.time()\n",
    "index = pc.Index(index_name)\n",
    "end_connect_time = time.time()\n",
    "connect_duration = end_connect_time - start_connect_time\n",
    "\n",
    "mlflow.log_metric(\"pinecone_index_connection_duration_seconds\", connect_duration)\n",
    "print(f\"Connected to Pinecone index '{index_name}' in {connect_duration:.4f} seconds.\")\n",
    "\n",
    "# Optional: Log some basic info about the connected index (e.g., number of vectors)\n",
    "# This might require querying the index, which adds time, so consider if needed for every run.\n",
    "try:\n",
    "    index_info = index.describe_index_stats()\n",
    "    mlflow.log_metric(\"pinecone_total_vector_count\", index_info.dimension) # This gives dimension, not vector count directly\n",
    "    # To get actual vector count:\n",
    "    if index_info.namespaces:\n",
    "        total_vectors_in_index = sum(ns.vector_count for ns_name, ns in index_info.namespaces.items())\n",
    "        mlflow.log_metric(\"pinecone_total_vectors_in_index\", total_vectors_in_index)\n",
    "        mlflow.log_param(\"pinecone_namespaces\", list(index_info.namespaces.keys()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not get Pinecone index stats: {e}\")\n",
    "    mlflow.log_param(\"pinecone_index_stats_error\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upsertion to Pinecone index 'medical-chatbot' with 3027 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsertion to Pinecone completed in 25.67 seconds.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# docsearch = PineconeVectorStore.from_documents(\n",
    "#     documents=text_chunks,\n",
    "#     embedding=embedding,\n",
    "#     index_name=index_name\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "\n",
    "# Log input parameters for this step\n",
    "mlflow.log_param(\"vector_store_type\", \"PineconeVectorStore\")\n",
    "mlflow.log_param(\"index_name_for_upsertion\", index_name) # Re-log index name for this specific step\n",
    "mlflow.log_param(\"num_text_chunks_for_upsertion\", len(text_chunks))\n",
    "\n",
    "# Assume embedding model name was already logged, if not, you might log it here again\n",
    "# mlflow.log_param(\"embedding_model_used_for_upsertion\", embedding.model_name) # if embedding object has a model_name attribute\n",
    "\n",
    "print(f\"Starting upsertion to Pinecone index '{index_name}' with {len(text_chunks)} chunks...\")\n",
    "\n",
    "start_time = time.time()\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name\n",
    ")\n",
    "end_time = time.time()\n",
    "upsertion_duration = end_time - start_time\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"pinecone_upsertion_duration_seconds\", upsertion_duration)\n",
    "\n",
    "print(f\"Upsertion to Pinecone completed in {upsertion_duration:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more data to the existing Pinecone vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe = Document(\n",
    "    page_content=\"Hi, My name is Gaurav singh. I am a software engineer. I am working on a project related to medical chatbot.\",\n",
    "    metadata={\"source\": \"gaurav\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d581ca8c-ca42-48a0-a7ec-736ecffafb0e']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.add_documents(documents=[swe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents for 'What is Cancer?' in 0.4099 seconds.\n"
     ]
    }
   ],
   "source": [
    "# ## Retrieve documents from Pinecone index as per the similarity\n",
    "\n",
    "# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "# retrieved_docs = retriever.invoke(\"What is Cancer?\")\n",
    "# retrieved_docs\n",
    "\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "import json\n",
    "import os # For cleaning up temporary artifact files\n",
    "\n",
    "\n",
    "# Define parameters for retrieval\n",
    "retrieval_search_type = \"similarity\"\n",
    "retrieval_k = 3\n",
    "test_query = \"What is Cancer?\"\n",
    "\n",
    "# 1. Log Retrieval Parameters\n",
    "mlflow.log_param(\"retriever_search_type\", retrieval_search_type)\n",
    "mlflow.log_param(\"retriever_k_value\", retrieval_k)\n",
    "mlflow.log_param(\"retrieval_query\", test_query) # Log the specific query used\n",
    "\n",
    "# Configure the retriever\n",
    "retriever = docsearch.as_retriever(search_type=retrieval_search_type, search_kwargs={\"k\": retrieval_k})\n",
    "\n",
    "# 2. Measure and Log Retrieval Time\n",
    "start_time = time.time()\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "end_time = time.time()\n",
    "retrieval_duration = end_time - start_time\n",
    "mlflow.log_metric(\"retrieval_duration_seconds\", retrieval_duration)\n",
    "\n",
    "# 3. Log Retrieved Document Count\n",
    "num_retrieved = len(retrieved_docs)\n",
    "mlflow.log_metric(\"num_retrieved_documents\", num_retrieved)\n",
    "\n",
    "# 4. Log a Sample of Retrieved Documents as an Artifact\n",
    "if retrieved_docs:\n",
    "    sample_docs = []\n",
    "    # Log details of the first 3 documents as a sample\n",
    "    for i, doc in enumerate(retrieved_docs[:3]):\n",
    "        sample_docs.append({\n",
    "            \"index\": i + 1,\n",
    "            \"source\": doc.metadata.get('source', 'N/A'),\n",
    "            \"page\": doc.metadata.get('page', 'N/A'),\n",
    "            \"content_preview\": doc.page_content[:200] + \"...\" # Log first 200 chars\n",
    "        })\n",
    "\n",
    "    temp_file_path = \"retrieved_docs_sample.json\"\n",
    "    with open(temp_file_path, \"w\") as f:\n",
    "        json.dump(sample_docs, f, indent=4)\n",
    "\n",
    "    mlflow.log_artifact(temp_file_path, artifact_path=\"retrieval_output_samples\")\n",
    "    os.remove(temp_file_path) # Clean up the local temporary file\n",
    "\n",
    "print(f\"Retrieved {num_retrieved} documents for '{test_query}' in {retrieval_duration:.4f} seconds.\")\n",
    "# You can also print a preview of retrieved_docs here if needed for immediate feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChatGoogleGenerativeAI model: gemini-2.0-flash...\n",
      "ChatGoogleGenerativeAI model initialized in 0.0363 seconds.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# chatModel = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.0-flash\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Define LLM parameters\n",
    "llm_model_name = \"gemini-2.0-flash\"\n",
    "llm_temperature = 0\n",
    "llm_max_tokens = None\n",
    "llm_timeout = None\n",
    "llm_max_retries = 2\n",
    "\n",
    "# Log LLM parameters\n",
    "mlflow.log_param(\"llm_provider\", \"Google_GenAI\")\n",
    "mlflow.log_param(\"llm_class\", \"ChatGoogleGenerativeAI\")\n",
    "mlflow.log_param(\"llm_model_name\", llm_model_name)\n",
    "mlflow.log_param(\"llm_temperature\", llm_temperature)\n",
    "mlflow.log_param(\"llm_max_tokens\", llm_max_tokens if llm_max_tokens is not None else \"None\") # Handle None gracefully\n",
    "mlflow.log_param(\"llm_timeout\", llm_timeout if llm_timeout is not None else \"None\") # Handle None gracefully\n",
    "mlflow.log_param(\"llm_max_retries\", llm_max_retries)\n",
    "\n",
    "\n",
    "print(f\"Initializing ChatGoogleGenerativeAI model: {llm_model_name}...\")\n",
    "start_time = time.time()\n",
    "chatModel = ChatGoogleGenerativeAI(\n",
    "    model=llm_model_name,\n",
    "    temperature=llm_temperature,\n",
    "    max_tokens=llm_max_tokens,\n",
    "    timeout=llm_timeout,\n",
    "    max_retries=llm_max_retries,\n",
    ")\n",
    "end_time = time.time()\n",
    "initialization_duration = end_time - start_time\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"llm_initialization_duration_seconds\", initialization_duration)\n",
    "\n",
    "print(f\"ChatGoogleGenerativeAI model initialized in {initialization_duration:.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template defined and logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "# system_prompt = (\n",
    "#     \"You are an Medical assistant for question-answering tasks. \"\n",
    "#     \"Use the following pieces of retrieved context to answer \"\n",
    "#     \"the question. If you don't know the answer, say that you \"\n",
    "#     \"don't know. Use five sentences maximum and keep the \"\n",
    "#     \"answer concise.\"\n",
    "#     \"\\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system_prompt),\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import hashlib # For creating a hash of the prompt string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an Medical assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use five sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create the ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 1. Log the system prompt string as a parameter\n",
    "mlflow.log_param(\"system_prompt_template\", system_prompt)\n",
    "\n",
    "# 2. Log the overall prompt template structure or its string representation\n",
    "# You can convert the ChatPromptTemplate to a string for logging\n",
    "full_prompt_template_str = str(prompt)\n",
    "mlflow.log_param(\"full_chat_prompt_template\", full_prompt_template_str)\n",
    "\n",
    "# 3. Log a hash of the full prompt template string for easy version comparison\n",
    "prompt_hash = hashlib.sha256(full_prompt_template_str.encode('utf-8')).hexdigest()\n",
    "mlflow.log_param(\"full_prompt_template_hash\", prompt_hash)\n",
    "\n",
    "# 4. Log the input variables used in the prompt\n",
    "mlflow.log_param(\"prompt_input_variables\", prompt.input_variables)\n",
    "\n",
    "\n",
    "# Optional: Save the prompt template to a file as an artifact\n",
    "# This can be useful if your prompts become very long or complex\n",
    "prompt_file_name = \"chat_prompt_template.txt\"\n",
    "with open(prompt_file_name, \"w\") as f:\n",
    "    f.write(full_prompt_template_str)\n",
    "mlflow.log_artifact(prompt_file_name, artifact_path=\"prompt_templates\")\n",
    "os.remove(prompt_file_name) # Clean up local file\n",
    "\n",
    "print(\"Prompt template defined and logged to MLflow.\")\n",
    "# --- MLflow Integration End ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating question-answer chain...\n",
      "Creating retrieval chain...\n",
      "Invoking RAG chain for query: 'What is Acne?'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain Response for 'What is Acne?':\n",
      "Acne vulgaris is a skin condition, and there are therapies for it.\n",
      "\n",
      "Time taken for RAG chain: 2.5657 seconds.\n"
     ]
    }
   ],
   "source": [
    "# question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "# rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# response = rag_chain.invoke({\"input\": \"What is Acne?\"})\n",
    "# print(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the input query for this run\n",
    "rag_input_query = \"What is Acne?\"\n",
    "\n",
    "# Log the input query for the RAG chain\n",
    "mlflow.log_param(\"rag_chain_input_query\", rag_input_query)\n",
    "\n",
    "\n",
    "print(\"Creating question-answer chain...\")\n",
    "Youtube_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "\n",
    "print(\"Creating retrieval chain...\")\n",
    "rag_chain = create_retrieval_chain(retriever, Youtube_chain)\n",
    "\n",
    "print(f\"Invoking RAG chain for query: '{rag_input_query}'...\")\n",
    "start_time = time.time()\n",
    "response = rag_chain.invoke({\"input\": rag_input_query})\n",
    "end_time = time.time()\n",
    "rag_chain_duration = end_time - start_time\n",
    "\n",
    "# Log overall RAG chain execution duration\n",
    "mlflow.log_metric(\"rag_chain_execution_duration_seconds\", rag_chain_duration)\n",
    "\n",
    "# Log the generated answer\n",
    "generated_answer = response.get(\"answer\", \"No answer found.\")\n",
    "mlflow.log_param(\"generated_answer\", generated_answer) # Log the full answer\n",
    "\n",
    "# Log the retrieved source documents as an artifact\n",
    "source_documents = response.get(\"context\", []) # 'context' typically holds source documents\n",
    "if source_documents:\n",
    "    retrieved_context_details = []\n",
    "    for i, doc in enumerate(source_documents):\n",
    "        retrieved_context_details.append({\n",
    "            \"index\": i + 1,\n",
    "            \"source\": doc.metadata.get('source', 'N/A'),\n",
    "            \"page\": doc.metadata.get('page', 'N/A'),\n",
    "            \"content_preview\": doc.page_content[:300] + \"...\" # Log first 300 chars\n",
    "        })\n",
    "\n",
    "    context_artifact_path = \"rag_chain_retrieved_context.json\"\n",
    "    with open(context_artifact_path, \"w\") as f:\n",
    "        json.dump(retrieved_context_details, f, indent=4)\n",
    "    mlflow.log_artifact(context_artifact_path, artifact_path=\"rag_chain_outputs\")\n",
    "    os.remove(context_artifact_path) # Clean up local file\n",
    "\n",
    "    mlflow.log_metric(\"num_context_documents_in_chain\", len(source_documents))\n",
    "else:\n",
    "    mlflow.log_metric(\"num_context_documents_in_chain\", 0)\n",
    "\n",
    "\n",
    "print(f\"RAG Chain Response for '{rag_input_query}':\")\n",
    "print(generated_answer)\n",
    "print(f\"\\nTime taken for RAG chain: {rag_chain_duration:.4f} seconds.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acromegaly and gigantism are conditions related to growth hormone excess.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Wwhat is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry, but the provided text does not contain information about the treatment of Acromegaly and gigantism.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is the treatment of Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medical/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided text does not contain any information about a person named Gaurav. The text focuses on Alexander's journey to improve his coordination and overcome vocal problems through self-observation and redirecting his thoughts.\n"
     ]
    }
   ],
   "source": [
    "##Since my RAG have some info about myself. So, I can ask about myself\n",
    "response = rag_chain.invoke({\"input\": \"Who is Gaurav?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
